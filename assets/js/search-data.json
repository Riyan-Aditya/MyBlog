{
  
    
        "post0": {
            "title": "[DRAFT - DO NOT SHARE] nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I got from a skeptic to a big fan of literate programming in a month? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://riyan-aditya.github.io//MyBlog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  
    
        ,"post5": {
            "title": "Classification tutorial following titanic dataset",
            "content": "This project is based on a YouTube video by KenJee The author also provided a link to Kaggle for his notebook This is also based on the popular Kaggle’s Titanic dataset which is used as the introduction for classification problem. . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Ken provided a good overview in each notebook session as well as introductory comment for each cell. Often I read the comment then attempted to code by myself first, then checking with his code afterwards. Good progress for my learning. . Introduction . This Titanic dataset is commonly used as the introduction into the Kaggle competition. With this dataset, we create a model to predict which passengers survived the Titanic shipwreck. . Coding process: . Import data | EDA | Feature Engineering | Data Preprocessing Dropped null values, used dummy variables, imputed data with median and using standard scaler | . | Model building Using: Naive Bayers, Logistic Regression, Decision Tree, K Nearest Neighbour, Random Forest, Support Vector Classifier, Extreme Gradient Boosting, Soft Voting Classifier | . | Model tuning | . Result: . From the base model, the cross validation score achieved was around 75 to 80%. The best model seems to be the Support Vector Classifier model. The ensemble method seems to perform better. . The models were later tuned by using GridSearch. Hyperparameter for model such as RandomForest may take ages to run. Overall, the model performance slightly improved (1 to 3%) after tuning. The Extreme Gradient Boosting model received the highest improvement. Some voting classifier were also conducted using combinations of the tuned models. . At the end, the best performance model with the Kaggle’s test dataset is the hard voting of the tuned model. This achieved 79% score with the test set. . . What I learnt . Insights: . Good idea to do EDA separately between numerical and categorical data. | For categorical data such as cabin or ticket, don’t be afraid to group variables based on certain category (eg: first letter of ticket). This can shorten combinations significantly. | Using Pandas’ get_dummies vs OneHotEncoding. Generally for ML, OHE seems to be better and can be used in pipelines | get_dummies can be applied to a dataframe automatically (it will automatically applied only for the categorical part of the DF). | . | For model like RandomForest, maybe better to start with RandomisedSearchCV then tune with GridSearchCV to cut down searching costs. | . Next step? . I am going to continue with Chapter 4 of the Geron’s book .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/21/Titanic-Kaggle.html",
            "relUrl": "/2020/09/21/Titanic-Kaggle.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Classification tutorial from Chapter 3 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 3 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Minor progress. In managed to become independent while following through the example notebook compared to when I was going through Project 1 (I tried to type some of the codes from scratch). . Introduction . The dataset is the MNIST data that are commonly used as the “hello world” for image classification. Basically, given the input, we are trying to train a model and compare it to the feature label (which is the digit of the image). . . Coding process: . Load data | Start from binary classifier. | Multiclass classifier. | Briefly cover multi label and multi output classification | . Problem: Some of the model takes forever to train (maybe my limited hardware). I ended up using sub datasets (up to 5k data) from the training datasets (60k). Hence, my output wasn’t optimal compared to the example notebook. . Result: Independent of the example, I attempted to make the final prediction by using the SGD model (trained with 10k data) and applied to the test set (the last 1k data imported). . The following is the confusion matrix plot of the test set. Bright colour indicates missclassifed. We can observe that the model confuse: . 3 and 5 (bright white box). | 7 and 9 | Furthermore, many numbers are miss classified as an 8 (colum with an 8). | . . What I learnt . Insights: . For classification problem, it is important to shuffle the dataset. Some algorithm may perform poorly if they get many instances in a row But this is not good for data such as time series data | . | Binary classification using SGDClassifier &amp; RandomForest =&gt; build a number 5 classifier . Performance metrics: accuracy, confusion matrix, precision, recall, F1, Precision Recall curve, ROC curve. . | Consider precision/recall tradeoff. They go together. High precision is not useful for low recall | Use PR curve instead of ROC curve when positive class is rare or care more about FP than FN | . | Multiclass classification =&gt; used SVC and SGD Assess with confusion matrix | Using matshow() can help to visualise and give insight on which classes that are easily miss-classified | . | . Next step? . I am buying this book and will continue on Chapter 4 next week. Really liked the explanation as well as the accompanying notebook in Github. . One of the added exercise is to go through the Titanic dataset on Kaggle. I watched a YouTube video on it before, tried to follow through but unfinished. I will re-attempt this again .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/18/Classification-ML-practice.html",
            "relUrl": "/2020/09/18/Classification-ML-practice.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "End to End ML Project tutorial from Chapter 2 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 2 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. But 95% of the work was following Geron’s. My follow along notebook is here. . Introduction . The dataset we are using is the California Housing Prices dataset based on 1990 California census (see Figure below). We were trying to predict the mean house prices in each district by using regression. . . In summary, it is an end to end ML project: . Importing data from Github | Exploratory Data Analysis | Preparing data Includes: imputing, dealing with categorical data, custom transformer, and applying pipelines | Select and train model The example used: linear regression, decision tree, random forest and support vector regression | Fine tune model / hyperparameter adjustment Via: GridSearch and Randomised search | . Results: . . As seen above, the lowest RMSE is for RandomForest with GridSearch tuning. With this model, the RMSE of the test set is $47.7k . What I learnt . What do i say. I learnt a lot as this is my first time going through the whole process. . Insights: . Random test-train split may not be good if the data is skewed. Test data should have a similar “sub-group” distribution as the full data set =&gt; use StratifiedShuffleSplit | Experiment with attributes. Feature engineering may create a better metrics Eg: In this example, number_of_bedrooms and number_of_household didn’t correlate well with median_house_value. Once we create number_of_rooms/household =&gt; this metric much more correlated to median_house_value | Using SimpleImputer | Using OrdinalEncoder vs OneHotEncoder | Feature engineering via custom transformer | Using Feature Scaling such as StandardScaler | Sparse vs dense matrix. Sparse matrix in OneHotEncoder due to the presence of many 0’s. | Applied the above via pipeline | Using K-fold cross validation (cross_val_score). Beware of training time. | Can do automatic hyperparameter via GridSearchCV and RandomisedSearchCV | Sometimes, knowing the final RMSE isn’t enough. How do we know if this performs better compared to an already deployed model ==&gt; might be good to find the 95% CI | . What I found confusing in this tutorial: . I found it confusing for the author to rename housing to strat_train_set. I would prefer to keep the “train set” variable label throughout the exercise. | . Next step? . Probably continue for classification problem tutorial | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/17/End-to-end-ML-project-practice.html",
            "relUrl": "/2020/09/17/End-to-end-ML-project-practice.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "ML roadmap (By Daniel Bourke)",
            "content": "I like this roadmap from Daniel Bourke’s Youtube video. The roadmap can be accessed here. . . I think this roadmap provides a good overview of how to learn ML. One-stop-shop for everything. The roadmap covers the basic explanation of ML, the available tools and resources and link to the maths behind them if required. . I also like how the video encourages you to approach the learning as a “cook” rather than a “chemist”. Start small. Step-by-step. Learn by doing rather than understanding everything in detail. . Quick overview . Basically, machine learning gives the ability for a machine to learn without being explicitly programmed. . Problems with long lists of rules Eg: it will be complex to program a self-driving car via traditional programming . | Continually changing environment Eg: self-driving car can adapt if there is a new road or traffic sign . | Discovering insights within large collections of data Eg: it will be too much to go to every single transaction manually if you are Amazon . | ML problems . Types of learning for ML: . Supervised learning You have data and labels. The model tries to learn the relationship between data and label. . | Unsupervised learning You have data but no labels. The model tries to find patterns in data without something to reference on. . | Transfer learning Take an existing ML model, then adjust it on your own and use it for your own problem. . | Reinforcement learning When an agent perform an action and being rewarded or penalised based on if the action is favourable or not . | ML problem domains: . Classification The model will use training dataset to learn and then use them to best map the input to the output/label. Eg: classify a mail as spam or not spam . | Regression The model will identify the relationship between the dependent and independent variables. Eg: the price of a stock over time . | Sequence-to-sequence Usually in languages for translation. Eg: Given a sequence in English, translate it to Spanish. . | Clustering Typically an unsupervised problem. Where the model groups data points based on similarity. Eg: Sort a soccer player based on their attributes (striker/defender/goalkeeper etc) . | Dimensionality reduction If you have so much input (100 variables), find the 10 most important variables. Eg: by using PCA (principal component analysis) . | ML process . First, you need to collect some data. It is important here to recognise the type of data you need. Data can be structured data in a table (eg: categorical, numerical, ordinal data etc) or unstructured data (eg: images, speech etc). Remember, rubbish in, rubbish out. . Second, you need to prepare the data. Typical data preparation steps: . Exploratory data analysis (EDA) This process involves understanding your data. Including exploring whether there are outliers and missing data. . | Data pre-processing This step is to prepare your data before the modelling process. Do you fill missing values? | Do we need to do feature encoding (changing categorical data into numbers)? | Do we need to do feature normalisation? | Do we need to do feature engineering? | Do we need to deal with data imbalances? | . | Data splitting For training (70-80% data), validation (10-15% data) and test set (10-15% data) as needed. It is important to not use the test set to tune the model. | . The third step is to train the model. . This is done by choosing an algorithm based on your problem and data. | Beware of underfitting (when the model doesn’t perform as well as you would like based on performance metrics) and overfitting (when the model performs far better on the training set than on the test set) the model. | Typically we overfit first, then reduce through various regularisation technique. | Finally, we can tune various hyperparameters by using validation set data. Eg of hyperparameters: learning rate (usually most important), number of layers, batch size, number of trees etc | . Next, we evaluate the model based on available metrics. There are several considerations here: . How long does a model take to train? | How long does a model need to predict? Eg: It is no good to have a self-driving car model that is 99% accurate but takes 15 seconds to make a prediction | Consider scenario such as what happens if the input data changes | Find out the least confident examples. What does the model usually get wrong? | Consider bias &amp; variance trade-off | . Once we are confident, we can serve the model. We will not know how it performs until we put it out for real. Use different tools whether the final goal is an a mobile app or a web based application. . Finally, we need to continue evaluating the model and retrain the model if needed. The model may change if the data source has changed (such as new road) or data source has been upgraded (such as new hardware used). . What have I learnt from this roadmap? . Good news. I know the basic. I have used Python before and familiar with the libraries. I guess I am closer to intermediate in ML. I have watched or read or have done tutorial covering some of these ML algorithms. . I guess I will start here: . . To do lists (probably in this order): . Do the 3 example projects | Learn/try how to use streamlit to deploy a basic proof of concept | Go through the fast.ai curriculum (seems to be highly recommended by DB, the author) | Test my knowledge in workera.ai | Projects, projects and projects. Probably using Kaggle | SQL maybe? | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/16/ML-roadmap.html",
            "relUrl": "/2020/09/16/ML-roadmap.html",
            "date": " • Sep 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riyan-aditya.github.io//MyBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riyan-aditya.github.io//MyBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}