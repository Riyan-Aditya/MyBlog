{
  
    
        "post0": {
            "title": "[DRAFT - DO NOT SHARE] nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I got from a skeptic to a big fan of literate programming in a month? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://riyan-aditya.github.io//MyBlog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Auto create certificate and email",
            "content": "In my organisation, we host webinar from time to time. Participants are getting e-certificate for attending. Our admin usually created custom certificate for each person manually, then emailing them one by one. Well, this is inefficiency, especially considering we are going to host 6 events in November. . I believe various Customer Relationship Manager (CRM) software can do this easily, but I thought, why not help out the admins while sharpening my programming skill. . Original code was found in stack overflow: . https://stackoverflow.com/questions/59289531/sending-email-to-different-recipients-with-different-file-attachments-using-pyth | . Import python code . # import python library import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email import encoders from string import Template import pandas as pd import os . . Read participant list . First, we need to read an excel file containing a list of all participants&#39; name to be written for each certificate certificate, as well as their email address for the certificates to be emailed to. . # read the file containing the database with the mail and the corresponding file path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email&#39; os.chdir(path) filename = &quot;lists_trial_for_blog.xlsx&quot; e = pd.read_excel(filename) . . e.head() . Nomer Nama participan Email participan . 0 | 1 | aaaa | aaaa@gmail.com | . 1 | 2 | bbbb | bbbb@gmail.com | . 2 | 3 | cccc dddd | cccc_dddd@gmail.com | . 3 | 4 | eeee | eeee@gmail.com | . Generate e-certificate for each participant . Then I will read the certificate template using Python&#39;s Pillow library . Note: I removed the bottom half to hide the signature in the certificate. This is the certificate that we gave in one of our previous webinar. . Links: . ISED: https://ised-id.org/ | Akutuku: https://akutuku.id/c/ | . # read certificate template and generate certificate for each participant from PIL import Image, ImageDraw, ImageFont # load and create image object path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat&#39; os.chdir(path) image_rgba = Image.open(&#39;E-CERTIFICATE template nosignature.png&#39;) # convert rgba image to rgb (PDF needs RGB) image_rgb = Image.new(&#39;RGB&#39;, image_rgba.size, (255, 255, 255)) # white background image_rgb.paste(image_rgba, mask=image_rgba.split()[3]) # paste using alpha channel as mask image_rgb . . # get dimension of certificate width, height = image_rgb.size width, height . . (1991, 887) . this code is to fill the certificate with each participants&#39; name . # this code is to fill the certificate with each participants&#39; name # Initialise drawing content with the image object as background # Go through xlsx file of participants and write the participants name in each certificate # save certificate on pdf # load font font = ImageFont.truetype(&quot;times-ro.ttf&quot;, 100) # change path to a new folder path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat/sertifikat_tiap_peserta&#39; os.chdir(path) # create empty column in DF to refer to the filename of each certificate #e[&#39;sertifikal_filename&#39;] = &quot;&quot; for index, row in e.iterrows(): # copy image_rgb as certificate template cert_template = image_rgb.copy() # define image, font, message draw = ImageDraw.Draw(cert_template) # use certificate width, but custom height W, H = width, 1150 msg = e.loc[index][&#39;Nama participan&#39;] # find out the width and height of the text based on the defined font w, h = draw.textsize(msg, font = font) # draw text draw.text(((W-w)/2,(H-h)/2), msg, fill=&quot;black&quot;, font = font) # image # save image cert_template.save(msg+&quot;.pdf&quot;, &quot;PDF&quot;) # update dataframe for filename of each participants e.at[index,&#39;sertifikat filename&#39;] = msg+&quot;.pdf&quot; . . Print one certificate to check output . # print one certificate to check cert_template . . Sending automatic email . In this example, we will use gmail. In gmail, you need to enable permission to send email from outside sources. . See here: https://support.google.com/accounts/answer/6010255 . First, login to your email. Input email and password . # In this example we will use gmail context = ssl.create_default_context() server = smtplib.SMTP_SSL(&#39;smtp.gmail.com&#39;, 465,context=context) server.login(&#39;xxxxxxxx@gmail.com&#39;,&#39;YourPassword&#39;) . . (235, b&#39;2.7.0 Accepted&#39;) . e.head() . Nomer Nama participan Email participan sertifikat filename . 0 | 1 | aaaa | aaaa@gmail.com | aaaa.pdf | . 1 | 2 | bbbb | bbbb@gmail.com | bbbb.pdf | . 2 | 3 | cccc dddd | cccc_dddd@gmail.com | cccc dddd.pdf | . 3 | 4 | eeee | eeee@gmail.com | eeee.pdf | . Create function to automatically attach certificate and send email . # Create function to auto send email # change path import os path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat/sertifikat_tiap_peserta&#39; os.chdir(path) # Email parameter subject = &quot;E-certificate webinar ISED&quot; fromaddr=&#39;xxxxxxxx@gmail.com&#39; def auto_email(df): # iterate over each row of participants data for index, row in df.iterrows(): # Email body body = (&quot;&quot;&quot; Selamat pagi Bapak/Ibu &quot;&quot;&quot;+str(row[&quot;Nama participan&quot;])+&quot;&quot;&quot;, Berikut terlampir e-sertifikat atas kehadiran Bapak/Ibu dalam acara webinar kita. Terima kasih. Salam, Riyan (on behalf of team ISED) &quot;&quot;&quot;) # Email body print (row[&quot;Email participan&quot;]+&quot; &quot;+row[&quot;sertifikat filename&quot;]) msg = MIMEMultipart() msg[&#39;From&#39;] = fromaddr msg[&#39;Subject&#39;] = subject msg.attach(MIMEText(body, &#39;plain&#39;)) filename = row[&quot;sertifikat filename&quot;] toaddr = row[&quot;Email participan&quot;] attachment = open(row[&quot;sertifikat filename&quot;], &quot;rb&quot;) part = MIMEBase(&#39;application&#39;, &#39;octet-stream&#39;) part.set_payload((attachment).read()) encoders.encode_base64(part) part.add_header(&#39;Content-Disposition&#39;, &quot;attachment; filename= %s&quot; % filename) msg.attach(part) text = msg.as_string() server.sendmail(fromaddr, toaddr, text) print(&#39;&#39;) print(&quot;Emails sent successfully&quot;) . . Then, send the emails . auto_email(e) # dont forget to quit the server server.quit() . aaaa@gmail.com aaaa.pdf bbbb@gmail.com bbbb.pdf cccc_dddd@gmail.com cccc dddd.pdf eeee@gmail.com eeee.pdf Emails sent successfully . (221, b&#39;2.0.0 closing connection j20sm22750989pfd.40 - gsmtp&#39;) .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/10/13/auto-cert-email.html",
            "relUrl": "/2020/10/13/auto-cert-email.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Covid19 in Indonesia",
            "content": "This notebook is my practise to do EDA (exploratory data analysis) . I used to be interested on the daily covid data when covid hit Indonesia back in March and April. It had been a while, so let&#39;s have a look at them now . Data will be scrapped from: . https://github.com/CSSEGISandData/COVID-19 | Our World in Data (for Indonesia): https://github.com/owid/covid-19-data/tree/master/public/data | Covid data for Indonesia from SINTA (via KawalCOVID): http://sinta.ristekbrin.go.id/covid/datasets | GEOjson for Indonesia: https://bitbucket.org/rifani/geojson-political-indonesia/src/master/ | . Summary of this notebook: . Download Covid19 data for worldwide and Indonesia from several sources | Download GEOjson map for Indonesia | Worldwide: summary of latest data, worldwide map, death vs confirmed comparison | Indonesia: summary of latest data, summary plots (total cases, daily cases, positive rate and mortality rate) and other random stats that I am interested in | . New skills I picked up and applied on this notebook: . First time using Git properly | Using Plotly Express | Extracting data from Google Sheet API | Cleaning data. The spreadsheet is messy. Table are stacked on other tables in the same spreadsheet tab | Extracted data is string. Not sure if there is a way to extract in a numeric format instead of converting it to float manually. For next time, maybe there is a way to just download from Google Sheet automatically and just pd.read_csv() | Working with GEOjson data format and plotting an interactive map | . Import necessary python libraries . # download python libraries from datetime import datetime, timedelta import os import glob import wget from bs4 import BeautifulSoup import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import geopandas as gpd import json import plotly.express as px import plotly.graph_objs as go # for offline ploting from plotly.offline import plot, iplot, init_notebook_mode init_notebook_mode(connected=True) from IPython.display import HTML . . Import data . # Download data from Github (daily) os.chdir(&quot;C:/Users/Riyan Aditya/Desktop/ML_learning/Project4_EDA_Covid_Indo/datasets&quot;) os.remove(&#39;time_series_covid19_confirmed_global.csv&#39;) os.remove(&#39;time_series_covid19_deaths_global.csv&#39;) os.remove(&#39;time_series_covid19_recovered_global.csv&#39;) # urls of the files urls = [&#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&#39;, &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv&#39;, &#39;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv&#39;] # download files for url in urls: filename = wget.download(url) . . 100% [............................................................................] 265849 / 265849 . Clean &amp; preprocess data . # convert csv to df confirmed_global = pd.read_csv(&#39;time_series_covid19_confirmed_global.csv&#39;) deaths_global = pd.read_csv(&#39;time_series_covid19_deaths_global.csv&#39;) recovered_global = pd.read_csv(&#39;time_series_covid19_recovered_global.csv&#39;) . . # Melt DF =&gt; switch rows of dates into column for simpler DF dates = confirmed_global.columns[4:] confirmed_globalv2 = confirmed_global.melt(id_vars = [&#39;Province/State&#39;, &#39;Country/Region&#39;, &#39;Lat&#39;, &#39;Long&#39;], value_vars = dates, var_name =&#39;Date&#39;, value_name = &#39;Confirmed&#39;) deaths_globalv2 = deaths_global.melt(id_vars = [&#39;Province/State&#39;, &#39;Country/Region&#39;, &#39;Lat&#39;, &#39;Long&#39;], value_vars = dates, var_name =&#39;Date&#39;, value_name = &#39;Deaths&#39;) recovered_globalv2 = recovered_global.melt(id_vars = [&#39;Province/State&#39;, &#39;Country/Region&#39;, &#39;Lat&#39;, &#39;Long&#39;], value_vars = dates, var_name =&#39;Date&#39;, value_name = &#39;Recovered&#39;) print(confirmed_globalv2.shape) print(deaths_globalv2.shape) print(recovered_globalv2.shape) . . (70755, 6) (70755, 6) (67310, 6) . Why are there differences in number of rows between confirmed (or death) &amp; recovered? . This seems to suggest some countries are missing their data . # Combine df covid_global = confirmed_globalv2.merge(deaths_globalv2, how=&#39;left&#39;, on = [&#39;Province/State&#39;, &#39;Country/Region&#39;, &#39;Lat&#39;, &#39;Long&#39;,&#39;Date&#39;]).merge( recovered_globalv2, how=&#39;left&#39;, on = [&#39;Province/State&#39;, &#39;Country/Region&#39;, &#39;Lat&#39;, &#39;Long&#39;,&#39;Date&#39;]) . . # preprocessing covid_global[&#39;Date&#39;] = pd.to_datetime(covid_global[&#39;Date&#39;]) #active cases covid_global[&#39;Active&#39;] = covid_global[&#39;Confirmed&#39;] - covid_global[&#39;Deaths&#39;] - covid_global[&#39;Recovered&#39;] . . Grouby data by day . # Data by day covid_global_daily = covid_global.groupby(&#39;Date&#39;)[&#39;Confirmed&#39;,&#39;Deaths&#39;,&#39;Recovered&#39;,&#39;Active&#39;].sum().reset_index() . . Grouby data by country . # Data by country temp = covid_global[covid_global[&#39;Date&#39;] ==max(covid_global[&#39;Date&#39;])].reset_index(drop=True).drop(&#39;Date&#39;, axis = 1) covid_global_percountry = temp.groupby(&#39;Country/Region&#39;)[&#39;Confirmed&#39;,&#39;Deaths&#39;,&#39;Recovered&#39;,&#39;Active&#39;].sum().reset_index() . . Worldwide Data Viz . Latest data . Show latest data . # latest data print(&#39;Date today&#39;,covid_global_daily[&#39;Date&#39;].iloc[-1]) print(&#39;Total cases&#39;,&#39;{:,}&#39;.format(covid_global_daily[&#39;Confirmed&#39;].iloc[-1])) print(&#39;Active cases&#39;,&#39;{:,}&#39;.format(covid_global_daily[&#39;Active&#39;].iloc[-1])) print(&#39;Recovered cases&#39;,&#39;{:,}&#39;.format(covid_global_daily[&#39;Recovered&#39;].iloc[-1])) print(&#39;Deaths cases&#39;,&#39;{:,}&#39;.format(covid_global_daily[&#39;Deaths&#39;].iloc[-1])) . . Date today 2020-10-12 00:00:00 Total cases 37,801,526 Active cases 9,207,268.0 Recovered cases 26,108,249.0 Deaths cases 995,057.0 . We almost reach 1M global death =( . # plot temp = covid_global_daily[[&#39;Date&#39;,&#39;Deaths&#39;,&#39;Recovered&#39;,&#39;Active&#39;]].tail(1) temp = temp.melt(id_vars=&#39;Date&#39;,value_vars = [&#39;Active&#39;,&#39;Deaths&#39;,&#39;Recovered&#39;]) fig = px.treemap(temp, path=[&#39;variable&#39;],values = &#39;value&#39;, height = 225) fig.data[0].textinfo = &#39;label+text+value&#39; . . HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Total confirmed cases world map . World map interactive plot . def plot_map(df, col, pal): df = df[df[col]&gt;0] fig2 = px.choropleth(df, locations=&quot;Country/Region&quot;, locationmode=&#39;country names&#39;, color=col, hover_name=&quot;Country/Region&quot;, title=col, hover_data=[col], color_continuous_scale=pal) fig2.update_layout(coloraxis_showscale=False) return fig2 . . fig2 = plot_map(covid_global_percountry, &#39;Confirmed&#39;, &#39;matter&#39;) HTML(fig2.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Treemap total confirmed cases . def plot_treemap(df,col): fig3 = px.treemap(df, path=[&quot;Country/Region&quot;], values=col, height=700, title=col, color_discrete_sequence = px.colors.qualitative.Dark2) fig3.data[0].textinfo = &#39;label+text+value&#39; return fig3 . . fig3 = plot_treemap(covid_global_percountry,&#39;Confirmed&#39;) HTML(fig3.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Death vs confirmed . For top 50 countries with the highest total cases . def human_format(num): magnitude = 0 while abs(num) &gt;= 1000: magnitude += 1 num /= 1000.0 # add more suffixes if you need them return &#39;%.2f%s&#39; % (num, [&#39;&#39;, &#39;K&#39;, &#39;M&#39;, &#39;G&#39;, &#39;T&#39;, &#39;P&#39;][magnitude]) . . # plot fig4 = px.scatter(covid_global_percountry.sort_values(&#39;Deaths&#39;, ascending=False).iloc[:50, :], x=&#39;Confirmed&#39;, y=&#39;Deaths&#39;, color=&#39;Country/Region&#39;, size=&#39;Confirmed&#39;, height=700, text=&#39;Country/Region&#39;, log_x=True, log_y=True, title=&#39;Deaths vs Confirmed (Scale is in log10)&#39;, hover_data={&#39;Country/Region&#39;:True,&#39;Confirmed&#39;:&#39;:,&#39;,&#39;Deaths&#39;:&#39;:,&#39;}) fig4.update_traces(textposition=&#39;top center&#39;) fig4.update_layout(showlegend=False) HTML(fig4.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Worldwide tests per thousand . Load data from Our World in Data . # load data url = &#39;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv&#39; df2 = pd.read_csv(url) . . C: Users Riyan Aditya Anaconda3 lib site-packages IPython core interactiveshell.py:3058: DtypeWarning: Columns (1,24) have mixed types. Specify dtype option on import or set low_memory=False. . Parse to latest data . # parse to latest data df2_latest = df2.loc[df2.date == date_to_parse].reset_index() df2_latest = df2_latest[:-2] . . Eliminate countries that don&#39;t report new tests per thousand, sort by new tests per thousand value. . Note: not all countries report their new tests data regularly . Number of countries that report their data: . # eliminate countries that dont report new tests per thousand, sort by new tests per thousand value df3 = df2_latest[~df2_latest[&#39;new_tests_per_thousand&#39;].isnull()] df3 = df3.loc[:, [&#39;location&#39;,&#39;new_tests_per_thousand&#39;]] df3 = df3.sort_values(by=[&#39;new_tests_per_thousand&#39;], ascending = False) df3.shape . . (41, 2) . Top 10 countries best on tests per thousand: . df3 = df3.reset_index() df3.head(10) . . index location new_tests_per_thousand . 0 | 6 | United Arab Emirates | 13.542 | . 1 | 20 | Bahrain | 5.167 | . 2 | 161 | Russia | 2.809 | . 3 | 123 | Maldives | 2.372 | . 4 | 159 | Qatar | 2.232 | . 5 | 118 | Latvia | 1.773 | . 6 | 175 | Slovakia | 1.641 | . 7 | 11 | Austria | 1.534 | . 8 | 189 | Turkey | 1.296 | . 9 | 85 | Croatia | 1.215 | . And Indonesia: . df3[df3[&#39;location&#39;]==&#39;Indonesia&#39;] . . index location new_tests_per_thousand . 32 | 88 | Indonesia | 0.081 | . we only tests 8% of our total population. Certainly much lower compared to other countries . Indonesia . Data starts 18 Mar . Import data . # find how many days in between 18 Mar to today (so we know how many rows to download) from datetime import date d0_total = date(2020, 3, 18) d0_harian = date(2020,3,15) d0_aktif = date(2020,3,21) d0_sembuh = date(2020,3,21) d0_deaths = date(2020,3,18) d1 = date.today() delta_total = d1 - d0_total delta_harian = d1 - d0_harian delta_aktif = d1 - d0_aktif delta_sembuh = d1 - d0_sembuh delta_deaths = d1 - d0_deaths . . All df imported are strings. need to convert to int. Also had to remove all the comma in the thousand separator. . Note: you need &quot;Credentials.json&quot;. You can get one by following this example: https://developers.google.com/sheets/api/quickstart/python . Code to import data from KawalCOVID: . # %load GoogleSheet from KawalCovid19 from __future__ import print_function import pickle import os.path from googleapiclient.discovery import build from google_auth_oauthlib.flow import InstalledAppFlow from google.auth.transport.requests import Request # If modifying these scopes, delete the file token.pickle. SCOPES = [&#39;https://www.googleapis.com/auth/spreadsheets.readonly&#39;] # The ID and range of a sample spreadsheet. SAMPLE_SPREADSHEET_ID = &#39;1ma1T9hWbec1pXlwZ89WakRk-OfVUQZsOCFl4FwZxzVw&#39; def main(): &quot;&quot;&quot;Shows basic usage of the Sheets API. Prints values from a sample spreadsheet. &quot;&quot;&quot; creds = None # The file token.pickle stores the user&#39;s access and refresh tokens, and is # created automatically when the authorization flow completes for the first # time. if os.path.exists(&#39;token.pickle&#39;): with open(&#39;token.pickle&#39;, &#39;rb&#39;) as token: creds = pickle.load(token) # If there are no (valid) credentials available, let the user log in. if not creds or not creds.valid: if creds and creds.expired and creds.refresh_token: creds.refresh(Request()) else: flow = InstalledAppFlow.from_client_secrets_file( &#39;credentials.json&#39;, SCOPES) creds = flow.run_local_server(port=0) # Save the credentials for the next run with open(&#39;token.pickle&#39;, &#39;wb&#39;) as token: pickle.dump(creds, token) service = build(&#39;sheets&#39;, &#39;v4&#39;, credentials=creds) # Call the Sheets API for total kasus - sheet = service.spreadsheets() SAMPLE_RANGE_NAME = &#39;Timeline!A1:AZ&#39; print(SAMPLE_RANGE_NAME) result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME).execute() values = result.get(&#39;values&#39;, []) df1 = pd.DataFrame(values) # Call the Sheets API for Statistik harian - sheet = service.spreadsheets() SAMPLE_RANGE_NAME = &#39;Statistik Harian!A1:AL&#39; print(SAMPLE_RANGE_NAME) result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME).execute() values = result.get(&#39;values&#39;, []) df = pd.DataFrame(values) headers = df.iloc[0] covid_id = pd.DataFrame(df.values[1:], columns=headers) covid_id.columns.values[0] = &quot;Dates&quot; covid_id = covid_id.replace(&#39;,&#39;,&#39;&#39;, regex=True).replace(&#39;-&#39;,&#39; &#39;, regex=True) covid_id = covid_id.replace(&#39;&#39;,0, regex=True) covid_id = covid_id.replace(&#39;#DIV/0!&#39;,0, regex=True) covid_id = covid_id.replace(&#39;#REF!&#39;,0, regex=True) covid_id = covid_id.set_index(&#39;Dates&#39;) covid_id = covid_id.replace(&#39;%&#39;,&#39;&#39;,regex=True).astype(&#39;float&#39;)/100 covid_id = covid_id.astype(&#39;float&#39;)*100 covid_id.index = pd.to_datetime(covid_id.index, format=&#39;%d %b&#39;) covid_id.index = covid_id.index + pd.DateOffset(year=2020) # Call the Sheets API for Population - sheet = service.spreadsheets() SAMPLE_RANGE_NAME = &#39;Copy of Data PCR - Kemenkes!J2:M36&#39; print(SAMPLE_RANGE_NAME) result = sheet.values().get(spreadsheetId=SAMPLE_SPREADSHEET_ID, range=SAMPLE_RANGE_NAME).execute() values = result.get(&#39;values&#39;, []) df2 = pd.DataFrame(values) # return DF -- return df1, covid_id, df2 indo_covid, covid_id, pop_id = main() . . Timeline!A1:AZ Statistik Harian!A1:AL . C: Users Riyan Aditya Anaconda3 lib site-packages pandas core arrays datetimes.py:837: PerformanceWarning: Non-vectorized DateOffset being applied to Series or DatetimeIndex . Copy of Data PCR - Kemenkes!J2:M36 . Clean data . indo_covid2 = indo_covid.copy() indo_covid2 = indo_covid2.iloc[:, :-1] . . # find index of relevant tables index_tk = indo_covid2.loc[indo_covid2[indo_covid2.columns[0]] == &#39;Total Kasus&#39;].index[0] index_ka = indo_covid2.loc[indo_covid2[indo_covid2.columns[0]] == &#39;Kasus Aktif&#39;].index[0] index_ks = indo_covid2.loc[indo_covid2[indo_covid2.columns[0]] == &#39;Sembuh&#39;].index[0] index_km = indo_covid2.loc[indo_covid2[indo_covid2.columns[0]] == &#39;Meninggal Dunia&#39;].index[0] #index_tk, index_ka, index_ks, index_km . . #Split table into total cases, active cases, recovered cases and deaths #Then set dates as the index total_cases = indo_covid2[index_tk:index_tk+delta_total.days+1:].rename( columns=indo_covid2.iloc[0]).drop(indo_covid2.index[0]).replace(&#39;-&#39;,&#39; &#39;, regex=True).set_index(&#39;Total Kasus&#39;) active_cases = indo_covid2[index_ka:index_ka+delta_aktif.days+1:].rename( columns=indo_covid2.iloc[index_ka]).drop(indo_covid2.index[index_ka]).replace(&#39;-&#39;,&#39; &#39;, regex=True).set_index(&#39;Kasus Aktif&#39;) recovered_cases = indo_covid2[index_ks:index_ks+delta_sembuh.days+1:].rename( columns=indo_covid2.iloc[index_ks]).drop(indo_covid2.index[index_ks]).replace(&#39;-&#39;,&#39; &#39;, regex=True).set_index(&#39;Sembuh&#39;) deaths_cases = indo_covid2[index_km:index_km+delta_deaths.days+1:].rename( columns=indo_covid2.iloc[index_km]).drop(indo_covid2.index[index_km]).replace(&#39;-&#39;,&#39; &#39;, regex=True).set_index(&#39;Meninggal Dunia&#39;) . . # clean df def clean_df(df): mask = df.applymap(lambda x: x is None) cols = df.columns[(mask).any()] for col in df[cols]: df.loc[mask[col], col] = 0 df = df.replace(&#39;,&#39;,&#39;&#39;, regex=True).replace(&#39;&#39;,0, regex=True) df = df.astype(&#39;float64&#39;) df.index = pd.to_datetime(df.index, format=&#39;%d %b&#39;) df.index = df.index + pd.DateOffset(year=2020) return df total_cases = clean_df(total_cases) active_cases = clean_df(active_cases) recovered_cases = clean_df(recovered_cases) deaths_cases = clean_df(deaths_cases) . . # generate new cases, new recovered, new deaths new_cases = total_cases.diff() new_recovered = recovered_cases.diff() new_deaths = deaths_cases.diff() . . Latest Indonesian data . # display latest data print(d1) print(&#39;Total cases&#39;,&#39;{:,}&#39;.format(covid_id[&#39;Total kasus&#39;][-1])) print(&#39;Active cases&#39;,&#39;{:,}&#39;.format(covid_id[&#39;Kasus aktif&#39;][-1])) print(&#39;Total recovered&#39;,&#39;{:,}&#39;.format(covid_id[&#39;Sembuh&#39;][-1])) print(&#39;Total deaths&#39;,&#39;{:,}&#39;.format(covid_id[&#39;Meninggal nDunia&#39;][-1])) . . 2020-10-13 Total cases 340,622.0 Active cases 65,299.0 Total recovered 263,296.0 Total deaths 12,027.0 . Total cases plot . Latest Indonesian Covid data: . # codes import matplotlib.ticker as mtick from matplotlib.dates import DateFormatter from pandas.plotting import * register_matplotlib_converters() fig, ax = plt.subplots(1, 1, figsize=(15,5)) ax.plot(covid_id.index,covid_id[&#39;Total kasus&#39;],&#39;-D&#39;,markersize = 3) plt.yticks(np.arange(0,400001,100000), fontsize=16) plt.xticks( fontsize=16) plt.ylabel(&#39;Total cases&#39;, fontsize=18) ax.get_yaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), &#39;,&#39;))) myFmt = DateFormatter(&quot;%b&quot;) ax.xaxis.set_major_formatter(myFmt) plt.show() . . Daily cases . # codes fig, ax = plt.subplots(1, 1, figsize=(15,5)) ln1 = ax.plot(covid_id.index,covid_id[&#39;Kasus baru&#39;],&#39;-D&#39;,markersize = 3,label = &#39;Daily cases&#39;) ax2 = ax.twinx() ln2 = ax2.bar(covid_id.index,covid_id[&#39;Spesimen&#39;],alpha = 0.2, color = &#39;black&#39;,label = &#39;Daily tests&#39;) ax.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=16) ax2.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=16) ax2.set_ylim([0,60000]) ax.get_yaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), &#39;,&#39;))) ax2.get_yaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), &#39;,&#39;))) ax.set_ylabel(&#39;Daily cases&#39;,fontsize = 18) ax2.set_ylabel(&#39;Daily tests&#39;,fontsize = 18) myFmt = DateFormatter(&quot;%b&quot;) ax.xaxis.set_major_formatter(myFmt) #legend fig.legend(bbox_to_anchor=(0.2,1), bbox_transform=ax.transAxes, fontsize = 16, frameon=False) plt.show() . . Unsurprisingly, daily cases are highly correlated with number of daily tests . Positive rate . This is a 7 day rolling average of new cases divided by number of people that are tested. . About 16% of the population tested returned a positive Covid test result. . # codes import matplotlib.ticker as mtick from matplotlib.dates import DateFormatter fig, ax = plt.subplots(1, 1, figsize=(15,5)) avg_pos_rate = covid_id[&#39;Positive rate harian&#39;].mean() ax.plot(covid_id.index,covid_id[&#39;Positive rate mingguan&#39;],&#39;-D&#39;,markersize = 3) plt.axhline(y=avg_pos_rate, color=&#39;k&#39;, linestyle=&#39;--&#39;, alpha = 0.3) plt.text(pd.to_datetime(&quot;2020-05-01&quot;),avg_pos_rate+0.02,&quot;average positive rate: &quot;+str(round(avg_pos_rate,1))+&quot;%&quot;,fontsize = 14) ax.set_ylim([0,40]) plt.yticks(np.arange(0,41,10), fontsize=16) plt.xticks( fontsize=16) plt.ylabel(&#39;Weekly positive rate (%)&#39;, fontsize=18) ax.get_yaxis().set_major_formatter(mtick.FuncFormatter(lambda x, p: format(int(x), &#39;,&#39;))) myFmt = DateFormatter(&quot;%b&quot;) ax.xaxis.set_major_formatter(myFmt) plt.show() . . Mortality rate . Average mortality rate is 5.5. However, this is likely affected by the data collected during March. This seems to indicate data was not collected (or the data collection is not fully functional yet). More accurate perhaps is the recent mortality rate, which is around 3.6. . # codes #death_rate = df_indo[&#39;2020-03-1&#39;::].new_deaths/df_indo[&#39;2020-03-1&#39;::].new_cases*100 avg_death_rate = covid_id[&#39;Tingkat kematian (seluruh kasus)&#39;].mean() fig, ax = plt.subplots(1, 1, figsize=(15,5)) ax.plot(covid_id.index, covid_id[&#39;Tingkat kematian (seluruh kasus)&#39;] ,&#39;-D&#39;,color =&#39;b&#39;,markersize = 3,alpha = 0.5) plt.axhline(y=avg_death_rate, color=&#39;b&#39;, linestyle=&#39;--&#39;, alpha = 0.3) plt.text(pd.to_datetime(&quot;2020-08-01&quot;),avg_death_rate+0.5,&quot;average mortality rate: &quot;+str(round(avg_death_rate,1))+&#39;%&#39;,fontsize = 14,color=&#39;blue&#39;) plt.axhline(y=3.6, color=&#39;g&#39;, linestyle=&#39;--&#39;, alpha = 0.3) plt.text(pd.to_datetime(&quot;2020-04-01&quot;),3.2-1.3,&quot;Recent mortality rate?: &quot;+str(3.6)+&#39;%&#39;,fontsize = 14,color=&#39;g&#39;) plt.yticks(np.arange(0,15.1,5), fontsize=16) plt.xticks( fontsize=16) plt.ylabel(&#39;Mortality rate&#39;, fontsize=18) myFmt = DateFormatter(&quot;%b&quot;) ax.xaxis.set_major_formatter(myFmt) plt.show() . . Interactive Province Map . # Load GEOjson data with open(&#39;IDN_adm_1_province.json&#39;) as data_file: indo_map = json.load(data_file) . . # temp a = [] for x in range(len(indo_map[&#39;features&#39;])): y = indo_map[&quot;features&quot;][x][&#39;properties&#39;][&#39;NAME_1&#39;] a.append(y) . . # transpose total cases per province indo_cases = total_cases.tail(1).T.reset_index() indo_cases.columns = [&#39;Province&#39;,&#39;Total_cases&#39;] indo_cases = indo_cases[:-1] # rename Province based on JSON name indo_cases[&#39;Province&#39;] = [&#39;Aceh&#39;,&#39;Bali&#39;,&#39;Banten&#39;,&#39;Bangka-Belitung&#39;,&#39;Bengkulu&#39;,&#39;Yogyakarta&#39;,&#39;Jakarta Raya&#39;,&#39;Jambi&#39;, &#39;Jawa Barat&#39;,&#39;Jawa Tengah&#39;,&#39;Jawa Timur&#39;,&#39;Kalimantan Barat&#39;,&#39;Kalimantan Timur&#39;, &#39;Kalimantan Tengah&#39;,&#39;Kalimantan Selatan&#39;,&#39;Kalimantan Utara&#39;,&#39;Kepulauan Riau&#39;, &#39;Nusa Tenggara Barat&#39;,&#39;Sumatera Selatan&#39;,&#39;Sumatera Barat&#39;,&#39;Sulawesi Utara&#39;, &#39;Sumatera Utara&#39;,&#39;Sulawesi Tenggara&#39;,&#39;Sulawesi Selatan&#39;,&#39;Sulawesi Tengah&#39;,&#39;Lampung&#39;, &#39;Riau&#39;,&#39;Maluku Utara&#39;,&#39;Maluku&#39;,&#39;Irian Jaya Barat&#39;,&#39;Papua&#39;,&#39;Sulawesi Barat&#39;, &#39;Nusa Tenggara Timur&#39;,&#39;Gorontalo&#39;] # transpose new cases per province indo_cases2 = new_cases.tail(1).T.reset_index() indo_cases2.columns = [&#39;Province&#39;,&#39;New_cases&#39;] indo_cases2 = indo_cases2[:-1] #combine DF indo_cases[&#39;New_cases&#39;] = indo_cases2[&#39;New_cases&#39;] . . # plot map fig5 = px.choropleth(indo_cases, geojson=indo_map, locations=indo_cases[&#39;Province&#39;], color=indo_cases[&#39;Total_cases&#39;], # lifeExp is a column of gapminder color_continuous_scale=px.colors.sequential.Reds,featureidkey=&quot;properties.NAME_1&quot;) fig5.update_geos(fitbounds=&quot;locations&quot;) fig5.update_layout(title = &#39;Total cases per province&#39;) HTML(fig5.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . # plot map fig6 = px.choropleth(indo_cases, geojson=indo_map, locations=indo_cases[&#39;Province&#39;], color=indo_cases[&#39;New_cases&#39;], # lifeExp is a column of gapminder color_continuous_scale=px.colors.sequential.Reds,featureidkey=&quot;properties.NAME_1&quot;) fig6.update_geos(fitbounds=&quot;locations&quot;) fig6.update_layout(title = &#39;New cases per province (&#39;+str(d1)+&#39;)&#39;) fig6.update_layout(coloraxis_colorbar=dict(title=&#39;Daily cases&#39;)) HTML(fig6.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Both the total cases and new cases indicate that most cases are in Java. This is expected since Java is the island with the most dense population. Data from other sources also indicate that most Covid tests are being performed in Java (especially Jakarta and East Java). . Other stats in Indonesia . Top 10 total cases and daily cases (last data) . def plot_hbar_sidexside(df1, col1, n1, df2, col2, n2, hover_data=[]): from plotly.subplots import make_subplots fig7 = px.bar(df1.sort_values(col1).tail(n1), x=col1, y=&quot;Province&quot;, text=col1, orientation=&#39;h&#39;, width=700, hover_data=hover_data, color_discrete_sequence = px.colors.qualitative.Dark2) fig8 = px.bar(df2.sort_values(col2).tail(n2), x=col2, y=&quot;Province&quot;, text=col2, orientation=&#39;h&#39;, width=700, hover_data=hover_data, color_discrete_sequence = px.colors.qualitative.Dark2) trace1 = fig7[&#39;data&#39;][0] trace2 = fig8[&#39;data&#39;][0] fig9 = make_subplots(rows=1, cols=2, shared_xaxes=False) fig9.add_trace(trace1, row=1, col=1) fig9.update_layout(title=&#39;Provinces with the highest total cases Provinces with the highest daily cases (&#39;+str(d1)+&#39;)&#39;) fig9.add_trace(trace2, row=1, col=2) #fig2.update_layout(title=&#39;Violin per feature2&#39;) return fig9 fig9 = plot_hbar_sidexside(indo_cases,&#39;Total_cases&#39;,10,indo_cases,&#39;New_cases&#39;,10,) HTML(fig9.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . Cases vs population . Clean population related data . # cleaning population data pop_id2 = pop_id.copy() pop_id2 = pop_id2 .rename(columns=pop_id2 .iloc[0]).drop(pop_id2 .index[0]) pop_id2.columns = [&#39;Province&#39;,&#39;suspect&#39;,&#39;Jumlah PCR+TCM&#39;,&#39;Population&#39;] # rename province to make it the same as the GEOjson pop_id2.at[3,&#39;Province&#39;] = &#39;Bangka-Belitung&#39; pop_id2.at[6,&#39;Province&#39;] = &#39;Yogyakarta&#39; pop_id2.at[7,&#39;Province&#39;] = &#39;Jakarta Raya&#39; pop_id2.at[25,&#39;Province&#39;] = &#39;Irian Jaya Barat&#39; # convert to numeric pop_id2 = pop_id2.replace(&#39;,&#39;,&#39;&#39;, regex=True).replace(&#39;&#39;,0, regex=True) pop_id2[&#39;suspect&#39;] = pd.to_numeric(pop_id2[&#39;suspect&#39;]) pop_id2[&#39;Jumlah PCR+TCM&#39;] = pd.to_numeric(pop_id2[&#39;Jumlah PCR+TCM&#39;]) pop_id2[&#39;Population&#39;] = pd.to_numeric(pop_id2[&#39;Population&#39;]) indo_cases = indo_cases.merge(pop_id2,how=&#39;left&#39;, left_on=&#39;Province&#39;, right_on=&#39;Province&#39;) . . # plot fig10 = px.scatter(indo_cases, y=&#39;Total_cases&#39;, x=&#39;Population&#39;, color=&#39;Total_cases&#39;, size=&#39;Total_cases&#39;, height=700, text=&#39;Province&#39;, log_x=True, log_y=True, title=&#39;Total cases vs Population (Scale is in log10)&#39;, hover_data={&#39;Province&#39;:True,&#39;Total_cases&#39;:&#39;:,&#39;,&#39;Population&#39;:&#39;:,&#39;}) fig10.update_traces(textposition=&#39;top center&#39;) fig10.update_layout(showlegend=False) HTML(fig10.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . # Plot LBF fig, ax = plt.subplots(1, 1, figsize=(15,5)) from numpy.polynomial.polynomial import polyfit y,x = indo_cases.Total_cases,indo_cases.Population plt.plot(x,y,&#39;o&#39;) b,m = polyfit(x, y, 1) plt.plot(x.sort_values(), m*x.sort_values() + b) jkt_data = indo_cases[indo_cases[&#39;Province&#39;]==&#39;Jakarta Raya&#39;] plt.xlabel(&#39;Population&#39;,fontsize = 18) plt.ylabel(&#39;Total cases&#39;,fontsize = 18) plt.yticks(np.arange(0,80001,20000), fontsize=16) plt.xticks(np.arange(0,50000001,10000000), fontsize=16) ax.annotate(&#39;Jakarta&#39;, xy=(jkt_data[&#39;Population&#39;]+1000000, jkt_data[&#39;Total_cases&#39;]), xycoords=&#39;data&#39;, xytext=(0.38, 0.975), textcoords=&#39;axes fraction&#39;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05, width = 0.4, headwidth = 10), horizontalalignment=&#39;right&#39;, verticalalignment=&#39;top&#39;,fontsize = 16 ) plt.show() . . That&#39;s weird. Jakarta has a significantly higher total cases relative to its population (a proxy for density) . Unless, if jakarta tests the proportion of their population much higher compared to other provinces? . Unfortunately, I cannot find data regarding tests per province. However, there is an article below. Based on the data there, Jakarta ran 35k tests per 1M population. Three times as much as the second place, Bali (12k tests / 1M population). Hence it might be possible the high covid rate in Jakarta per population is due to the number of testing here . https://www.thejakartapost.com/news/2020/07/22/testing-disparity-among-regions-a-challenge-for-covid-19-response.html . Java vs non Java . Create java and non-java category. . # create java and non-java category indo_cases[&#39;Java&#39;] = &#39;Non-Java&#39; indo_cases.at[[2,5,6,8,9,10],&#39;Java&#39;] = indo_cases[&#39;Province&#39;] . . # plot from plotly.subplots import make_subplots fig11 = px.pie(indo_cases, values =&#39;Total_cases&#39;, names = &#39;Java&#39;,color=&#39;Java&#39;, color_discrete_sequence=px.colors.sequential.Teal) fig12 = (px.pie(indo_cases, values =&#39;New_cases&#39;, names = &#39;Java&#39; ,title = &#39;Total cases&#39;)) trace1 = fig11[&#39;data&#39;][0] trace2 = fig12[&#39;data&#39;][0] fig13 = make_subplots(rows=1, cols=2, specs=[[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;}]]) fig13.add_trace(trace1, row=1, col=1) fig13.add_trace(trace2, row=1, col=2) fig13.update_traces(textposition=&#39;inside&#39;, textinfo=&#39;percent+label&#39;) fig13.update_traces(hole =.4,hoverinfo=&quot;label+percent+name&quot;) fig13.update_traces(textposition = &quot;outside&quot;) fig13.update_layout(showlegend=False) fig13.update_layout(title_text=&quot;Total cases and new cases (&quot;+str(d1)+&#39;)&#39; ,annotations =[dict(text=&#39;Total&#39;, x=0.18, y=0.5, font_size=20, showarrow=False), dict(text=&#39;New&#39;, x=0.82, y=0.5, font_size=20, showarrow=False)]) HTML(fig13.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . . It can be observed that 60% of the reported Covid19 cases in Indonesia is in Java. The trend still continues for the new cases reported daily . Effect of PSBB? . PSBB: . Full PSBB: 10 April - 4 Juni | PSBB transisi: 5 Juni - 10 September | Full PSBB: 11 September - 12 Oktober | PSBB transisi: 12 Oktober - etc | . #np.log(covid_id[&#39;Kasus baru&#39;]).plot() #np.log(new_cases[&#39;Jakarta&#39;]).plot() .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/10/12/CovidID.html",
            "relUrl": "/2020/10/12/CovidID.html",
            "date": " • Oct 12, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://riyan-aditya.github.io//MyBlog/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riyan-aditya.github.io//MyBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riyan-aditya.github.io//MyBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}