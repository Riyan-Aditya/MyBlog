{
  
    
        "post0": {
            "title": "[DRAFT - DO NOT SHARE] nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I got from a skeptic to a big fan of literate programming in a month? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://riyan-aditya.github.io//MyBlog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Auto create certificate and email",
            "content": "In my organisation, we host webinar from time to time. Participants are getting e-certificate for attending. Our admin usually created custom certificate for each person manually, then emailing them one by one. Well, this is inefficiency, especially considering we are going to host 6 events in November. . I believe various Customer Relationship Manager (CRM) software can do this easily, but I thought, why not help out the admins while sharpening my programming skill. . Original code was found in stack overflow: . https://stackoverflow.com/questions/59289531/sending-email-to-different-recipients-with-different-file-attachments-using-pyth | . Import python code . # import python library import smtplib, ssl from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart from email.mime.base import MIMEBase from email import encoders from string import Template import pandas as pd import os . . Read participant list . First, we need to read an excel file containing a list of all participants&#39; name to be written for each certificate certificate, as well as their email address for the certificates to be emailed to. . # read the file containing the database with the mail and the corresponding file path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email&#39; os.chdir(path) filename = &quot;lists_trial_for_blog.xlsx&quot; e = pd.read_excel(filename) . . e.head() . Nomer Nama participan Email participan . 0 | 1 | aaaa | aaaa@gmail.com | . 1 | 2 | bbbb | bbbb@gmail.com | . 2 | 3 | cccc dddd | cccc_dddd@gmail.com | . 3 | 4 | eeee | eeee@gmail.com | . Generate e-certificate for each participant . Then I will read the certificate template using Python&#39;s Pillow library . Note: I removed the bottom half to hide the signature in the certificate. This is the certificate that we gave in one of our previous webinar. . Links: . ISED: https://ised-id.org/ | Akutuku: https://akutuku.id/c/ | . # read certificate template and generate certificate for each participant from PIL import Image, ImageDraw, ImageFont # load and create image object path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat&#39; os.chdir(path) image_rgba = Image.open(&#39;E-CERTIFICATE template nosignature.png&#39;) # convert rgba image to rgb (PDF needs RGB) image_rgb = Image.new(&#39;RGB&#39;, image_rgba.size, (255, 255, 255)) # white background image_rgb.paste(image_rgba, mask=image_rgba.split()[3]) # paste using alpha channel as mask image_rgb . . # get dimension of certificate width, height = image_rgb.size width, height . . (1991, 887) . this code is to fill the certificate with each participants&#39; name . # this code is to fill the certificate with each participants&#39; name # Initialise drawing content with the image object as background # Go through xlsx file of participants and write the participants name in each certificate # save certificate on pdf # load font font = ImageFont.truetype(&quot;times-ro.ttf&quot;, 100) # change path to a new folder path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat/sertifikat_tiap_peserta&#39; os.chdir(path) # create empty column in DF to refer to the filename of each certificate #e[&#39;sertifikal_filename&#39;] = &quot;&quot; for index, row in e.iterrows(): # copy image_rgb as certificate template cert_template = image_rgb.copy() # define image, font, message draw = ImageDraw.Draw(cert_template) # use certificate width, but custom height W, H = width, 1150 msg = e.loc[index][&#39;Nama participan&#39;] # find out the width and height of the text based on the defined font w, h = draw.textsize(msg, font = font) # draw text draw.text(((W-w)/2,(H-h)/2), msg, fill=&quot;black&quot;, font = font) # image # save image cert_template.save(msg+&quot;.pdf&quot;, &quot;PDF&quot;) # update dataframe for filename of each participants e.at[index,&#39;sertifikat filename&#39;] = msg+&quot;.pdf&quot; . . Print one certificate to check output . # print one certificate to check cert_template . . Sending automatic email . In this example, we will use gmail. In gmail, you need to enable permission to send email from outside sources. . See here: https://support.google.com/accounts/answer/6010255 . First, login to your email. Input email and password . # In this example we will use gmail context = ssl.create_default_context() server = smtplib.SMTP_SSL(&#39;smtp.gmail.com&#39;, 465,context=context) server.login(&#39;xxxxxxxx@gmail.com&#39;,&#39;YourPassword&#39;) . . (235, b&#39;2.7.0 Accepted&#39;) . e.head() . Nomer Nama participan Email participan sertifikat filename . 0 | 1 | aaaa | aaaa@gmail.com | aaaa.pdf | . 1 | 2 | bbbb | bbbb@gmail.com | bbbb.pdf | . 2 | 3 | cccc dddd | cccc_dddd@gmail.com | cccc dddd.pdf | . 3 | 4 | eeee | eeee@gmail.com | eeee.pdf | . Create function to automatically attach certificate and send email . # Create function to auto send email # change path import os path = &#39;C:/Users/Riyan Aditya/Desktop/ML_learning/Project5_ISED_email/sertifikat/sertifikat_tiap_peserta&#39; os.chdir(path) # Email parameter subject = &quot;E-certificate webinar ISED&quot; fromaddr=&#39;xxxxxxxx@gmail.com&#39; def auto_email(df): # iterate over each row of participants data for index, row in df.iterrows(): # Email body body = (&quot;&quot;&quot; Selamat pagi Bapak/Ibu &quot;&quot;&quot;+str(row[&quot;Nama participan&quot;])+&quot;&quot;&quot;, Berikut terlampir e-sertifikat atas kehadiran Bapak/Ibu dalam acara webinar kita. Terima kasih. Salam, Riyan (on behalf of team ISED) &quot;&quot;&quot;) # Email body print (row[&quot;Email participan&quot;]+&quot; &quot;+row[&quot;sertifikat filename&quot;]) msg = MIMEMultipart() msg[&#39;From&#39;] = fromaddr msg[&#39;Subject&#39;] = subject msg.attach(MIMEText(body, &#39;plain&#39;)) filename = row[&quot;sertifikat filename&quot;] toaddr = row[&quot;Email participan&quot;] attachment = open(row[&quot;sertifikat filename&quot;], &quot;rb&quot;) part = MIMEBase(&#39;application&#39;, &#39;octet-stream&#39;) part.set_payload((attachment).read()) encoders.encode_base64(part) part.add_header(&#39;Content-Disposition&#39;, &quot;attachment; filename= %s&quot; % filename) msg.attach(part) text = msg.as_string() server.sendmail(fromaddr, toaddr, text) print(&#39;&#39;) print(&quot;Emails sent successfully&quot;) . . Then, send the emails . auto_email(e) # dont forget to quit the server server.quit() . aaaa@gmail.com aaaa.pdf bbbb@gmail.com bbbb.pdf cccc_dddd@gmail.com cccc dddd.pdf eeee@gmail.com eeee.pdf Emails sent successfully . (221, b&#39;2.0.0 closing connection j20sm22750989pfd.40 - gsmtp&#39;) .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/10/13/auto-cert-email.html",
            "relUrl": "/2020/10/13/auto-cert-email.html",
            "date": " • Oct 13, 2020"
        }
        
    
  
    
  
    
        ,"post3": {
            "title": "End to End ML Project tutorial from Chapter 2 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 2 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. But 95% of the work was following Geron’s. . Introduction . The dataset we are using is the California Housing Prices dataset based on 1990 California census (see Figure below). We were trying to predict the mean house prices in each district by using regression. . . In summary, it is an end to end ML project: . Importing data from Github | Exploratory Data Analysis | Preparing data Includes: imputing, dealing with categorical data, custom transformer, and applying pipelines | Select and train model The example used: linear regression, decision tree, random forest and support vector regression | Fine tune model / hyperparameter adjustment Via: GridSearch and Randomised search | . Results: . . As seen above, the lowest RMSE is for RandomForest with GridSearch tuning. With this model, the RMSE of the test set is $47.7k . What I learnt . What do i say. I learnt a lot as this is my first time going through the whole process. . Insights: . Random test-train split may not be good if the data is skewed. Test data should have a similar “sub-group” distribution as the full data set =&gt; use StratifiedShuffleSplit | Experiment with attributes. Feature engineering may create a better metrics Eg: In this example, number_of_bedrooms and number_of_household didn’t correlate well with median_house_value. Once we create number_of_rooms/household =&gt; this metric much more correlated to median_house_value | Using SimpleImputer | Using OrdinalEncoder vs OneHotEncoder | Feature engineering via custom transformer | Using Feature Scaling such as StandardScaler | Sparse vs dense matrix. Sparse matrix in OneHotEncoder due to the presence of many 0’s. | Applied the above via pipeline | Using K-fold cross validation (cross_val_score). Beware of training time. | Can do automatic hyperparameter via GridSearchCV and RandomisedSearchCV | Sometimes, knowing the final RMSE isn’t enough. How do we know if this performs better compared to an already deployed model ==&gt; might be good to find the 95% CI | . What I found confusing in this tutorial: . I found it confusing for the author to rename housing to strat_train_set. I would prefer to keep the “train set” variable label throughout the exercise. | . Next step? . Probably continue for classification problem tutorial | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/17/End-to-end-ML-project-practice.html",
            "relUrl": "/2020/09/17/End-to-end-ML-project-practice.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "ML roadmap (By Daniel Bourke)",
            "content": "I like this roadmap from Daniel Bourke’s Youtube video. The roadmap can be accessed here. . . I think this roadmap provides a good overview of how to learn ML. One-stop-shop for everything. The roadmap covers the basic explanation of ML, the available tools and resources and link to the maths behind them if required. . I also like how the video encourages you to approach the learning as a “cook” rather than a “chemist”. Start small. Step-by-step. Learn by doing rather than understanding everything in detail. . Quick overview . Basically, machine learning gives the ability for a machine to learn without being explicitly programmed. . Problems with long lists of rules Eg: it will be complex to program a self-driving car via traditional programming . | Continually changing environment Eg: self-driving car can adapt if there is a new road or traffic sign . | Discovering insights within large collections of data Eg: it will be too much to go to every single transaction manually if you are Amazon . | ML problems . Types of learning for ML: . Supervised learning You have data and labels. The model tries to learn the relationship between data and label. . | Unsupervised learning You have data but no labels. The model tries to find patterns in data without something to reference on. . | Transfer learning Take an existing ML model, then adjust it on your own and use it for your own problem. . | Reinforcement learning When an agent perform an action and being rewarded or penalised based on if the action is favourable or not . | ML problem domains: . Classification The model will use training dataset to learn and then use them to best map the input to the output/label. Eg: classify a mail as spam or not spam . | Regression The model will identify the relationship between the dependent and independent variables. Eg: the price of a stock over time . | Sequence-to-sequence Usually in languages for translation. Eg: Given a sequence in English, translate it to Spanish. . | Clustering Typically an unsupervised problem. Where the model groups data points based on similarity. Eg: Sort a soccer player based on their attributes (striker/defender/goalkeeper etc) . | Dimensionality reduction If you have so much input (100 variables), find the 10 most important variables. Eg: by using PCA (principal component analysis) . | ML process . First, you need to collect some data. It is important here to recognise the type of data you need. Data can be structured data in a table (eg: categorical, numerical, ordinal data etc) or unstructured data (eg: images, speech etc). Remember, rubbish in, rubbish out. . Second, you need to prepare the data. Typical data preparation steps: . Exploratory data analysis (EDA) This process involves understanding your data. Including exploring whether there are outliers and missing data. . | Data pre-processing This step is to prepare your data before the modelling process. Do you fill missing values? | Do we need to do feature encoding (changing categorical data into numbers)? | Do we need to do feature normalisation? | Do we need to do feature engineering? | Do we need to deal with data imbalances? | . | Data splitting For training (70-80% data), validation (10-15% data) and test set (10-15% data) as needed. It is important to not use the test set to tune the model. | . The third step is to train the model. . This is done by choosing an algorithm based on your problem and data. | Beware of underfitting (when the model doesn’t perform as well as you would like based on performance metrics) and overfitting (when the model performs far better on the training set than on the test set) the model. | Typically we overfit first, then reduce through various regularisation technique. | Finally, we can tune various hyperparameters by using validation set data. Eg of hyperparameters: learning rate (usually most important), number of layers, batch size, number of trees etc | . Next, we evaluate the model based on available metrics. There are several considerations here: . How long does a model take to train? | How long does a model need to predict? Eg: It is no good to have a self-driving car model that is 99% accurate but takes 15 seconds to make a prediction | Consider scenario such as what happens if the input data changes | Find out the least confident examples. What does the model usually get wrong? | Consider bias &amp; variance trade-off | . Once we are confident, we can serve the model. We will not know how it performs until we put it out for real. Use different tools whether the final goal is an a mobile app or a web based application. . Finally, we need to continue evaluating the model and retrain the model if needed. The model may change if the data source has changed (such as new road) or data source has been upgraded (such as new hardware used). . What have I learnt from this roadmap? . Good news. I know the basic. I have used Python before and familiar with the libraries. I guess I am closer to intermediate in ML. I have watched or read or have done tutorial covering some of these ML algorithms. . I guess I will start here: . . To do lists (probably in this order): . Do the 3 example projects | Learn/try how to use streamlit to deploy a basic proof of concept | Go through the fast.ai curriculum (seems to be highly recommended by DB, the author) | Test my knowledge in workera.ai | Projects, projects and projects. Probably using Kaggle | SQL maybe? | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/16/ML-roadmap.html",
            "relUrl": "/2020/09/16/ML-roadmap.html",
            "date": " • Sep 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riyan-aditya.github.io//MyBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riyan-aditya.github.io//MyBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}