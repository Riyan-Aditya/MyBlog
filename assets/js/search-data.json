{
  
    
        "post0": {
            "title": "Import python module",
            "content": "Objective of this notebook: . Apply webscrap by myself | Visualise FIFA21 player data | Unsupervised learning to predict players&#39; position | . Note book summary: . Create python script (BeautifulSoup) to scrap FIFA21 EPL players data from https://sofifa.com/ | Clean data | Gained information from the data | . import pandas as pd import numpy as np import requests from bs4 import BeautifulSoup as bs import time import re from random import randrange . . Webscrap FIFA21 EPL players&#39; stats . First, check if you can webscrap. sofifa.com indicate you can webscrap through their robots.txt . Webscrap procedure: . Search prem players in Fifa21 | Results are stored in table, 60 per page. up to 600 | open each pages, grab the link to pages for individual players | . Note: scrap responsibly. Add a random 2-6 seconds delay in between access to webpages . Grab list of EPL players . links = [] for x in range(11): # make a search and scrap the webpage -- base_url = &#39;https://sofifa.com/players?type=all&amp;lg%5B%5D=13&#39; if x &gt; 0: add_url =&#39;&amp;offset=&#39;+str(x*60) else: add_url=&#39;&#39; r = requests.get(base_url+add_url) # call the soup function to make the html readable -- webpage = bs(r.content) table = webpage.find(&#39;table&#39;) rows = table.find_all(&#39;tr&#39;) for row in rows[1::]: link = row.find_all(&quot;a&quot;,{&quot;class&quot;:&quot;tooltip&quot;})[0] links.append(link[&#39;href&#39;]) # print progress on scrapping link print(&quot;Page&quot;,x,&quot;done&quot;) #Be sure to pause time.sleep(randrange(2,6)) . . Page 0 done Page 1 done Page 2 done Page 3 done Page 4 done Page 5 done Page 6 done Page 7 done Page 8 done Page 9 done Page 10 done . Number of players in EPL: . len(links) . . 643 . The top 10 players, sorted by overall rating: . links[:10] . . [&#39;/player/192985/kevin-de-bruyne/210010/&#39;, &#39;/player/212831/alisson-ramses-becker/210010/&#39;, &#39;/player/209331/mohamed-salah/210010/&#39;, &#39;/player/208722/sadio-mane/210010/&#39;, &#39;/player/203376/virgil-van-dijk/210010/&#39;, &#39;/player/153079/sergio-aguero/210010/&#39;, &#39;/player/215914/ngolo-kante/210010/&#39;, &#39;/player/210257/ederson-santana-de-moraes/210010/&#39;, &#39;/player/202652/raheem-sterling/210010/&#39;, &#39;/player/200104/heung-min-son/210010/&#39;] . Ok, we have 644 players registered in EPL in the FIFA21 game. That is 32 players per club which is about right . Scrap individual player stats . Note: I have scrapped them before using the same function below. For for this demo, I will only scrap the first 10 players . links3 = links[:10] . Players to scrap (show first 5): . links3[:5] . . [&#39;/player/192985/kevin-de-bruyne/210010/&#39;, &#39;/player/212831/alisson-ramses-becker/210010/&#39;, &#39;/player/209331/mohamed-salah/210010/&#39;, &#39;/player/208722/sadio-mane/210010/&#39;, &#39;/player/203376/virgil-van-dijk/210010/&#39;] . Use function to scrap . list_stats_top10 = [] def player_name(weblink): return weblink.find(&#39;h1&#39;).text def nat(weblink): return weblink.find(&#39;div&#39;,{&quot;class&quot;:&quot;meta bp3-text-overflow-ellipsis&quot;}).a[&#39;title&#39;] def dob_wh(weblink): stuff = weblink.find(&#39;div&#39;,{&quot;class&quot;:&quot;meta bp3-text-overflow-ellipsis&quot;}).text temp = stuff.split(&quot;(&quot;)[1] dob = temp.split(&quot;)&quot;)[0] temp2 = temp.split(&quot;)&quot;)[1].split(&quot; &quot;) height = temp2[1] weight = temp2[2] return dob, height, weight def club_info(weblink): club = weblink.find(text = re.compile(&#39;Player Specialities&#39;)).parent.findNext(&#39;h5&#39;).text jersey = weblink.find(text = re.compile(&#39;Jersey Number&#39;)).next c_valid = weblink.find(text = re.compile(&#39;Contract Valid Until&#39;)).next c_value = weblink.find(&#39;section&#39;,{&quot;class&quot;:&quot;card spacing&quot;}).find(text = re.compile(&#39;Value&#39;)).previous.previous wage = weblink.find(&#39;section&#39;,{&quot;class&quot;:&quot;card spacing&quot;}).find(text = re.compile(&#39;Wage&#39;)).previous.previous return club, jersey, c_valid, c_value, wage def player_stats(weblink): best_pos = weblink.find(text = re.compile(&#39;Best Position&#39;)).next.text best_rating = weblink.find(text = re.compile(&#39;Best Overall Rating&#39;)).next.text return best_pos, best_rating def player_stats_detail(weblink): # Attacking stats temp = weblink.find(text = re.compile(&#39;Attacking&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;Crossing&#39;,&#39;Finishing&#39;,&#39;Heading Accuracy&#39;,&#39;Short Passing&#39;,&#39;Volleys&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # skill stats temp = weblink.find(text = re.compile(&#39;Attacking&#39;)).parent.parent.parent.find_next_sibling(&quot;div&quot;).find_all(&quot;li&quot;) keys = [&#39;Dribbling&#39;,&#39;Curve&#39;,&#39;FK Accuracy&#39;,&#39;Long Passing&#39;,&#39;Ball Control&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # movement stats temp = weblink.find(text = re.compile(&#39;Movement&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;Acceleration&#39;,&#39;Spring Speed&#39;,&#39;Agility&#39;,&#39;Reactions&#39;,&#39;Balance&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # power stats temp = weblink.find(text = re.compile(&#39;Power&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;Shot Power&#39;,&#39;Jumping&#39;,&#39;Stamina&#39;,&#39;Strength&#39;,&#39;Long Shots&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # mentality stats temp = weblink.find(text = re.compile(&#39;Mentality&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;Aggression&#39;,&#39;Interceptions&#39;,&#39;Positioning&#39;,&#39;Vision&#39;,&#39;Penalties&#39;,&#39;Composure&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # defending stats temp = weblink.find(text = re.compile(&#39;Defending&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;Defensive Awareness&#39;,&#39;Standing Tackle&#39;,&#39;Sliding Tackle&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # goalkeeping stats temp = weblink.find(text = re.compile(&#39;Goalkeeping&#39;)).parent.parent.find_all(&quot;li&quot;) keys = [&#39;GK Diving&#39;,&#39;GK Handling&#39;,&#39;GK Kicking&#39;,&#39;GK Positioning&#39;,&#39;GK Reflexes&#39;] for index,attrs in enumerate(temp): temp2 = attrs.find_all(&#39;span&#39;) stats[keys[index]] = temp2[0].text # traits stats try: temp = weblink.find(text = re.compile(&#39;Traits&#39;)).parent.parent.find_all(&quot;li&quot;) for attrs in temp: if &#39;Traits&#39; in stats: stats[&#39;Traits&#39;].append(attrs.text) else: stats[&#39;Traits&#39;] = [attrs.text] except: stats[&#39;Traits&#39;] = None for index,link in enumerate(links3): # make a search and scrap the webpage -- base_url = &#39;https://sofifa.com/&#39; add_url = link r = requests.get(base_url+add_url) weblink = bs(r.content) stats={} # get player name name = player_name(weblink) stats[&#39;Player_name&#39;] = name # get nationality nationality = nat(weblink) stats[&#39;Nationality&#39;] = nationality # get dob, weight, height dob, height, weight = dob_wh(weblink) stats[&#39;dob&#39;] = dob stats[&#39;height&#39;] = height stats[&#39;weight&#39;] = weight # get club info club, jersey, c_valid, c_value, wage = club_info(weblink) stats[&#39;club&#39;] = club stats[&#39;jersey&#39;] = jersey stats[&#39;c_valid&#39;] = c_valid stats[&#39;c_value&#39;] = c_value stats[&#39;wage&#39;] = wage # add general player stats pos, rating = player_stats(weblink) stats[&#39;pos&#39;] = pos stats[&#39;rating&#39;] = rating # add player stats detail player_stats_detail(weblink) # print progress -- list_stats_top10.append(stats) if index % 10 ==0: print(index) #Be sure to pause between accessing pages time.sleep(randrange(2,6)) . . 0 . Save JSON data of FIFA21 EPL players . import json def save_data(title,data): with open(title, &#39;w&#39;, encoding =&#39;utf-8&#39;) as f: json.dump(data, f, ensure_ascii=False, indent = 2) def load_data(title): with open(title, encoding =&#39;utf-8&#39;) as f: return json.load(f) . . save_data(&#39;FIFA21_EPL_top10.json&#39;, list_stats_top10) . Load data for later use . FIFA21_data = load_data(&#39;FIFA21_EPL.json&#39;) . Lets see the first data, Kevin De Bruyne . FIFA21_data[0] . {&#39;Player_name&#39;: &#39;K. De Bruyne&#39;, &#39;Nationality&#39;: &#39;Belgium&#39;, &#39;dob&#39;: &#39;Jun 28, 1991&#39;, &#39;height&#39;: &#39;5 &#39;11&#34;&#39;, &#39;weight&#39;: &#39;154lbs&#39;, &#39;club&#39;: &#39;Manchester City&#39;, &#39;jersey&#39;: &#39;17&#39;, &#39;c_valid&#39;: &#39;2023&#39;, &#39;c_value&#39;: &#39;€129M&#39;, &#39;wage&#39;: &#39;€370K&#39;, &#39;pos&#39;: &#39;CAM&#39;, &#39;rating&#39;: &#39;91&#39;, &#39;Crossing&#39;: &#39;94&#39;, &#39;Finishing&#39;: &#39;82&#39;, &#39;Heading Accuracy&#39;: &#39;55&#39;, &#39;Short Passing&#39;: &#39;94&#39;, &#39;Volleys&#39;: &#39;82&#39;, &#39;Dribbling&#39;: &#39;88&#39;, &#39;Curve&#39;: &#39;85&#39;, &#39;FK Accuracy&#39;: &#39;83&#39;, &#39;Long Passing&#39;: &#39;93&#39;, &#39;Ball Control&#39;: &#39;92&#39;, &#39;Acceleration&#39;: &#39;77&#39;, &#39;Spring Speed&#39;: &#39;76&#39;, &#39;Agility&#39;: &#39;78&#39;, &#39;Reactions&#39;: &#39;91&#39;, &#39;Balance&#39;: &#39;76&#39;, &#39;Shot Power&#39;: &#39;91&#39;, &#39;Jumping&#39;: &#39;63&#39;, &#39;Stamina&#39;: &#39;89&#39;, &#39;Strength&#39;: &#39;74&#39;, &#39;Long Shots&#39;: &#39;91&#39;, &#39;Aggression&#39;: &#39;76&#39;, &#39;Interceptions&#39;: &#39;66&#39;, &#39;Positioning&#39;: &#39;88&#39;, &#39;Vision&#39;: &#39;94&#39;, &#39;Penalties&#39;: &#39;84&#39;, &#39;Composure&#39;: &#39;91&#39;, &#39;Defensive Awareness&#39;: &#39;68&#39;, &#39;Standing Tackle&#39;: &#39;65&#39;, &#39;Sliding Tackle&#39;: &#39;53&#39;, &#39;GK Diving&#39;: &#39;15&#39;, &#39;GK Handling&#39;: &#39;13&#39;, &#39;GK Kicking&#39;: &#39;5&#39;, &#39;GK Positioning&#39;: &#39;10&#39;, &#39;GK Reflexes&#39;: &#39;13&#39;, &#39;Traits&#39;: [&#39;Injury Prone&#39;, &#39;Leadership&#39;, &#39;Early Crosser&#39;, &#39;Long Passer (AI)&#39;, &#39;Long Shot Taker (AI)&#39;, &#39;Playmaker (AI)&#39;, &#39;Outside Foot Shot&#39;]} . Data cleaning . df = pd.DataFrame(FIFA21_data) . df.head() . Player_name Nationality dob height weight club jersey c_valid c_value wage ... Composure Defensive Awareness Standing Tackle Sliding Tackle GK Diving GK Handling GK Kicking GK Positioning GK Reflexes Traits . 0 | K. De Bruyne | Belgium | Jun 28, 1991 | 5&#39;11&quot; | 154lbs | Manchester City | 17 | 2023 | €129M | €370K | ... | 91 | 68 | 65 | 53 | 15 | 13 | 5 | 10 | 13 | [Injury Prone, Leadership, Early Crosser, Long... | . 1 | Alisson | Brazil | Oct 2, 1992 | 6&#39;3&quot; | 201lbs | Liverpool | 1 | 2024 | €102M | €160K | ... | 65 | 15 | 19 | 16 | 86 | 88 | 85 | 91 | 89 | [GK Long Throw, Rushes Out Of Goal] | . 2 | M. Salah | Egypt | Jun 15, 1992 | 5&#39;9&quot; | 157lbs | Liverpool | 11 | 2023 | €120.5M | €250K | ... | 90 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | [Finesse Shot, Long Shot Taker (AI), Speed Dri... | . 3 | S. Mané | Senegal | Apr 10, 1992 | 5&#39;9&quot; | 152lbs | Liverpool | 10 | 2023 | €120.5M | €250K | ... | 84 | 42 | 42 | 38 | 10 | 10 | 15 | 7 | 14 | [Flair, Speed Dribbler (AI)] | . 4 | V. van Dijk | Netherlands | Jul 8, 1991 | 6&#39;4&quot; | 203lbs | Liverpool | 4 | 2023 | €113M | €210K | ... | 90 | 93 | 93 | 86 | 13 | 10 | 13 | 11 | 11 | [Solid Player, Leadership, Long Passer (AI), P... | . 5 rows × 47 columns . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 643 entries, 0 to 642 Data columns (total 47 columns): Player_name 643 non-null object Nationality 643 non-null object dob 643 non-null object height 643 non-null object weight 643 non-null object club 643 non-null object jersey 643 non-null object c_valid 643 non-null object c_value 643 non-null object wage 643 non-null object pos 643 non-null object rating 643 non-null object Crossing 643 non-null object Finishing 643 non-null object Heading Accuracy 643 non-null object Short Passing 643 non-null object Volleys 643 non-null object Dribbling 643 non-null object Curve 643 non-null object FK Accuracy 643 non-null object Long Passing 643 non-null object Ball Control 643 non-null object Acceleration 643 non-null object Spring Speed 643 non-null object Agility 643 non-null object Reactions 643 non-null object Balance 643 non-null object Shot Power 643 non-null object Jumping 643 non-null object Stamina 643 non-null object Strength 643 non-null object Long Shots 643 non-null object Aggression 643 non-null object Interceptions 643 non-null object Positioning 643 non-null object Vision 643 non-null object Penalties 643 non-null object Composure 643 non-null object Defensive Awareness 643 non-null object Standing Tackle 643 non-null object Sliding Tackle 643 non-null object GK Diving 643 non-null object GK Handling 643 non-null object GK Kicking 643 non-null object GK Positioning 643 non-null object GK Reflexes 643 non-null object Traits 446 non-null object dtypes: object(47) memory usage: 236.2+ KB . Few things to do: . Everything is string. Convert to numeric when needed to be, especially the individual stats | Convert DOB to datetime | Convert height and weight to the SI unit | Convert value and wages to the right unit (eg: no M and no K) | What to do with traits in players? Perhaps ignore for now | . Traits . First, lets look at the traits . How many unique traits are there? | Proportion of players that have traits? | Is it worth keeping? | . Players with no traits: . df[&#39;Traits&#39;].isnull().values.ravel().sum() . . 197 . 199 from 644 players (~30%) do not have any traits . Top 5 players that have no traits: . df[df[&#39;Traits&#39;].isnull()][:5].Player_name . . 6 N. Kanté 32 Rodri 94 D. Sánchez 115 N. Matić 124 J. Evans Name: Player_name, dtype: object . Wow. Kante doesnt have any traits? This could be a mistake from Fifa21 database . Unique traits: . df.Traits.explode().unique() . . array([&#39;Injury Prone&#39;, &#39;Leadership&#39;, &#39;Early Crosser&#39;, &#39;Long Passer (AI)&#39;, &#39;Long Shot Taker (AI)&#39;, &#39;Playmaker (AI)&#39;, &#39;Outside Foot Shot&#39;, &#39;GK Long Throw&#39;, &#39;Rushes Out Of Goal&#39;, &#39;Finesse Shot&#39;, &#39;Speed Dribbler (AI)&#39;, &#39;Chip Shot (AI)&#39;, &#39;Flair&#39;, &#39;Solid Player&#39;, &#39;Power Header&#39;, None, &#39;Comes For Crosses&#39;, &#39;Team Player&#39;, &#39;Dives Into Tackles (AI)&#39;, &#39;Technical Dribbler (AI)&#39;, &#39;Cautious With Crosses&#39;, &#39;Saves with Feet&#39;, &#39;Long Throw-in&#39;, &#39;Power Free-Kick&#39;, &#39;Giant Throw-in&#39;, nan], dtype=object) . print(&quot;number of unique traits :&quot;,len(df.Traits.explode().unique())) . . number of unique traits : 26 . 26 unique traits. Probably too long if I expand the column similar to how a OHE works . Is it worth keeping? . Probably not. Remove them for now . df2 = df.copy() df2 = df2.drop(labels=&#39;Traits&#39;, axis=1) . . df2.columns . . Index([&#39;Player_name&#39;, &#39;Nationality&#39;, &#39;dob&#39;, &#39;height&#39;, &#39;weight&#39;, &#39;club&#39;, &#39;jersey&#39;, &#39;c_valid&#39;, &#39;c_value&#39;, &#39;wage&#39;, &#39;pos&#39;, &#39;rating&#39;, &#39;Crossing&#39;, &#39;Finishing&#39;, &#39;Heading Accuracy&#39;, &#39;Short Passing&#39;, &#39;Volleys&#39;, &#39;Dribbling&#39;, &#39;Curve&#39;, &#39;FK Accuracy&#39;, &#39;Long Passing&#39;, &#39;Ball Control&#39;, &#39;Acceleration&#39;, &#39;Spring Speed&#39;, &#39;Agility&#39;, &#39;Reactions&#39;, &#39;Balance&#39;, &#39;Shot Power&#39;, &#39;Jumping&#39;, &#39;Stamina&#39;, &#39;Strength&#39;, &#39;Long Shots&#39;, &#39;Aggression&#39;, &#39;Interceptions&#39;, &#39;Positioning&#39;, &#39;Vision&#39;, &#39;Penalties&#39;, &#39;Composure&#39;, &#39;Defensive Awareness&#39;, &#39;Standing Tackle&#39;, &#39;Sliding Tackle&#39;, &#39;GK Diving&#39;, &#39;GK Handling&#39;, &#39;GK Kicking&#39;, &#39;GK Positioning&#39;, &#39;GK Reflexes&#39;], dtype=&#39;object&#39;) . # fix mistake with column name df2.rename(columns={&#39;Spring Speed&#39;:&#39;Sprint Speed&#39;}, inplace=True) . . Convert to numeric . df2.head() . Player_name Nationality dob height weight club jersey c_valid c_value wage ... Penalties Composure Defensive Awareness Standing Tackle Sliding Tackle GK Diving GK Handling GK Kicking GK Positioning GK Reflexes . 0 | K. De Bruyne | Belgium | Jun 28, 1991 | 5&#39;11&quot; | 154lbs | Manchester City | 17 | 2023 | €129M | €370K | ... | 84 | 91 | 68 | 65 | 53 | 15 | 13 | 5 | 10 | 13 | . 1 | Alisson | Brazil | Oct 2, 1992 | 6&#39;3&quot; | 201lbs | Liverpool | 1 | 2024 | €102M | €160K | ... | 23 | 65 | 15 | 19 | 16 | 86 | 88 | 85 | 91 | 89 | . 2 | M. Salah | Egypt | Jun 15, 1992 | 5&#39;9&quot; | 157lbs | Liverpool | 11 | 2023 | €120.5M | €250K | ... | 83 | 90 | 38 | 43 | 41 | 14 | 14 | 9 | 11 | 14 | . 3 | S. Mané | Senegal | Apr 10, 1992 | 5&#39;9&quot; | 152lbs | Liverpool | 10 | 2023 | €120.5M | €250K | ... | 71 | 84 | 42 | 42 | 38 | 10 | 10 | 15 | 7 | 14 | . 4 | V. van Dijk | Netherlands | Jul 8, 1991 | 6&#39;4&quot; | 203lbs | Liverpool | 4 | 2023 | €113M | €210K | ... | 62 | 90 | 93 | 93 | 86 | 13 | 10 | 13 | 11 | 11 | . 5 rows × 46 columns . DOB to datetime . df2[&#39;dob&#39;] = pd.to_datetime(df2[&#39;dob&#39;]) . . df2.dob.dtype . dtype(&#39;&lt;M8[ns]&#39;) . Height to numeric &amp; SI . def height_conversion(ht): # current format is x&#39;x&quot; ht2 = ht.split(&quot;&#39;&quot;) ft = float(ht2[0]) inc = float(ht2[1].replace(&quot; &quot;&quot;,&quot;&quot;)) # &quot; is a special character return round(((12*ft)+inc)*2.54,0) df2[&#39;height&#39;] = df2[&#39;height&#39;].apply(lambda x:height_conversion(x)) . . df2.height.dtype . . dtype(&#39;float64&#39;) . Weight to numeric &amp; SI . def weight_conversion(wt): # current format is xxxlbs wt2 = wt.split(&#39;lbs&#39;) return round(float(wt2[0])*0.453592,0) df2[&#39;weight&#39;] = df2[&#39;weight&#39;].apply(lambda x:weight_conversion(x)) . . df2.weight.dtype . . dtype(&#39;float64&#39;) . Contract value and wage to numeric . value_dict = {&quot;K&quot;:1000, &quot;M&quot;:1000000 } def money_conversion(money): # current format is €xxxxK money = money.replace(&#39;€&#39;,&#39;&#39;) if money[-1] in value_dict: num, values = money[:-1], money[-1] return (float(num)* value_dict[values]) df2[&#39;c_value&#39;] = df2[&#39;c_value&#39;].apply(lambda x:money_conversion(x)) df2[&#39;wage&#39;] = df2[&#39;wage&#39;].apply(lambda x:money_conversion(x)) . . df2.c_value.dtype, df2.wage.dtype . . (dtype(&#39;float64&#39;), dtype(&#39;float64&#39;)) . Convert jersey, rating and individual stats attributes to numeric . df2[&#39;jersey&#39;] = pd.to_numeric(df2[&#39;jersey&#39;], errors=&#39;coerce&#39;) df2[&#39;rating&#39;] = pd.to_numeric(df2[&#39;rating&#39;], errors=&#39;coerce&#39;) for col in df2.columns[12::]: df2[col] = pd.to_numeric(df2[col], errors=&#39;coerce&#39;) . . df3 = df2.copy() . . Top players of Fifa21 . Top rated players from each club . Note that there can be multiple top rated players per club . idx = df3.groupby([&#39;club&#39;])[&#39;rating&#39;].transform(max) == df3[&#39;rating&#39;] top_rated = df3[idx][[&#39;club&#39;,&#39;Player_name&#39;,&#39;pos&#39;,&#39;rating&#39;]] top_rated.sort_values(&#39;club&#39;) . . club Player_name pos rating . 11 | Arsenal | P. Aubameyang | ST | 87 | . 105 | Aston Villa | J. Grealish | CAM | 83 | . 194 | Brighton &amp; Hove Albion | L. Trossard | CAM | 80 | . 129 | Brighton &amp; Hove Albion | M. Ryan | GK | 80 | . 193 | Brighton &amp; Hove Albion | L. Dunk | CB | 80 | . 77 | Burnley | N. Pope | GK | 82 | . 6 | Chelsea | N. Kanté | CDM | 88 | . 63 | Crystal Palace | W. Zaha | CF | 83 | . 51 | Everton | Allan | CDM | 85 | . 76 | Fulham | A. Areola | GK | 82 | . 84 | Leeds United | Raphinha | LM | 82 | . 68 | Leeds United | Rodrigo | ST | 82 | . 20 | Leicester City | J. Vardy | ST | 87 | . 45 | Leicester City | W. Ndidi | CB | 87 | . 1 | Liverpool | Alisson | GK | 90 | . 2 | Liverpool | M. Salah | RW | 90 | . 3 | Liverpool | S. Mané | LW | 90 | . 4 | Liverpool | V. van Dijk | CB | 90 | . 0 | Manchester City | K. De Bruyne | CAM | 91 | . 18 | Manchester United | Bruno Fernandes | CAM | 88 | . 59 | Newcastle United | M. Dúbravka | GK | 83 | . 156 | Sheffield United | S. Berge | CDM | 80 | . 187 | Sheffield United | J. Egan | CB | 80 | . 189 | Sheffield United | J. O&#39;Connell | CB | 80 | . 92 | Southampton | D. Ings | ST | 81 | . 145 | Southampton | J. Ward-Prowse | CM | 81 | . 10 | Tottenham Hotspur | H. Kane | ST | 88 | . 9 | Tottenham Hotspur | H. Son | LM | 88 | . 153 | West Bromwich Albion | B. Ivanović | CB | 79 | . 96 | West Ham United | T. Souček | CDM | 82 | . 97 | West Ham United | S. Haller | ST | 82 | . 47 | Wolverhampton Wanderers | R. Jiménez | ST | 84 | . 46 | Wolverhampton Wanderers | Rui Patrício | GK | 84 | . Top rated players per pos . These positions are based on what FIFA21 recommend as &quot;Best Position&quot; . idx = df3.groupby([&#39;pos&#39;])[&#39;rating&#39;].transform(max) == df3[&#39;rating&#39;] top_rated = df3[idx][[&#39;pos&#39;,&#39;club&#39;,&#39;Player_name&#39;,&#39;rating&#39;]] # create custom sort so this makes positional sense custom_dict = {&#39;GK&#39;:0, &#39;CB&#39;:1, &#39;LB&#39;:2, &#39;RB&#39;:3, &#39;LWB&#39;:4, &#39;RWB&#39;:5, &#39;CDM&#39;:6, &#39;CM&#39;:7, &#39;CAM&#39;:8, &#39;RM&#39;:9, &#39;LM&#39;:10, &#39;RW&#39;:11, &#39;LW&#39;:12, &#39;CF&#39;:13, &#39;ST&#39;:14} top_rated[&#39;rank&#39;] = top_rated[&#39;pos&#39;].map(custom_dict) top_pos = top_rated.sort_values(&#39;rank&#39;) top_pos.drop(labels=[&#39;rank&#39;], axis=1) . . pos club Player_name rating . 1 | GK | Liverpool | Alisson | 90 | . 4 | CB | Liverpool | V. van Dijk | 90 | . 13 | LB | Liverpool | A. Robertson | 87 | . 12 | RB | Liverpool | T. Alexander-Arnold | 87 | . 89 | LWB | Manchester City | B. Mendy | 82 | . 54 | RWB | Manchester City | João Cancelo | 84 | . 6 | CDM | Chelsea | N. Kanté | 88 | . 23 | CM | Manchester United | P. Pogba | 86 | . 50 | CM | Chelsea | M. Kovačić | 86 | . 0 | CAM | Manchester City | K. De Bruyne | 91 | . 34 | RM | Manchester United | M. Rashford | 86 | . 9 | LM | Tottenham Hotspur | H. Son | 88 | . 2 | RW | Liverpool | M. Salah | 90 | . 3 | LW | Liverpool | S. Mané | 90 | . 15 | CF | Liverpool | Roberto Firmino | 87 | . 5 | ST | Manchester City | S. Agüero | 89 | . Some weird result here. Right forward maybe, but I would not put Rasford as a Right milfielder . Top rated players per original country . Unique nationalities of Prem players . df3.Nationality.unique().shape . . (60,) . List of top players per nationality of origin: . idx = df3.groupby([&#39;Nationality&#39;])[&#39;rating&#39;].transform(max) == df3[&#39;rating&#39;] top_rated = df3[idx][[&#39;Nationality&#39;,&#39;club&#39;,&#39;Player_name&#39;,&#39;pos&#39;,&#39;rating&#39;]] top_rated.sort_values(&#39;Nationality&#39;) . . Nationality club Player_name pos rating . 33 | Algeria | Manchester City | R. Mahrez | RW | 85 | . 5 | Argentina | Manchester City | S. Agüero | ST | 89 | . 129 | Australia | Brighton &amp; Hove Albion | M. Ryan | GK | 80 | . 390 | Austria | Leicester City | C. Fuchs | CDM | 74 | . 0 | Belgium | Manchester City | K. De Bruyne | CAM | 91 | . ... | ... | ... | ... | ... | ... | . 112 | Ukraine | Manchester City | O. Zinchenko | CM | 81 | . 80 | United States | Chelsea | C. Pulisic | CAM | 82 | . 43 | Uruguay | Manchester United | E. Cavani | ST | 84 | . 60 | Wales | Tottenham Hotspur | G. Bale | RW | 83 | . 297 | Zimbabwe | Aston Villa | M. Nakamba | CDM | 76 | . 64 rows × 5 columns . Top player based on some stats . Best crosser . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Crossing&#39;]].loc[df3[&#39;Crossing&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Crossing 94 Name: 0, dtype: object . Best short passer . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Short Passing&#39;]].loc[df3[&#39;Short Passing&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Short Passing 94 Name: 0, dtype: object . Best long passer . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Long Passing&#39;]].loc[df3[&#39;Long Passing&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Long Passing 93 Name: 0, dtype: object . Best header . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Heading Accuracy&#39;]].loc[df3[&#39;Heading Accuracy&#39;].idxmax()] . . Player_name O. Giroud club Chelsea pos ST Heading Accuracy 90 Name: 130, dtype: object . Best finisher . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Finishing&#39;]].loc[df3[&#39;Finishing&#39;].idxmax()] . . Player_name S. Agüero club Manchester City pos ST Finishing 94 Name: 5, dtype: object . Best FK taker . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;FK Accuracy&#39;]].loc[df3[&#39;FK Accuracy&#39;].idxmax()] . . Player_name J. Ward-Prowse club Southampton pos CM FK Accuracy 91 Name: 145, dtype: object . Best PK taker . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Penalties&#39;]].loc[df3[&#39;Penalties&#39;].idxmax()] . . Player_name R. Jiménez club Wolverhampton Wanderers pos ST Penalties 92 Name: 47, dtype: object . Best volleyer . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Volleys&#39;]].loc[df3[&#39;Volleys&#39;].idxmax()] . . Player_name J. Rodríguez club Everton pos CAM Volleys 90 Name: 67, dtype: object . Highest shot power . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Shot Power&#39;]].loc[df3[&#39;Shot Power&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Shot Power 91 Name: 0, dtype: object . Speed merchant and acceleration . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Sprint Speed&#39;]].loc[df3[&#39;Sprint Speed&#39;].idxmax()] . . Player_name Adama Traoré club Wolverhampton Wanderers pos RM Sprint Speed 96 Name: 155, dtype: object . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Acceleration&#39;]].loc[df3[&#39;Acceleration&#39;].idxmax()] . . Player_name Adama Traoré club Wolverhampton Wanderers pos RM Acceleration 97 Name: 155, dtype: object . Dribbler . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Dribbling&#39;]].loc[df3[&#39;Dribbling&#39;].idxmax()] . . Player_name Bernardo Silva club Manchester City pos RW Dribbling 92 Name: 17, dtype: object . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Ball Control&#39;]].loc[df3[&#39;Ball Control&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Ball Control 92 Name: 0, dtype: object . Best stamina . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Stamina&#39;]].loc[df3[&#39;Stamina&#39;].idxmax()] . . Player_name N. Kanté club Chelsea pos CDM Stamina 96 Name: 6, dtype: object . Best strength . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Strength&#39;]].loc[df3[&#39;Strength&#39;].idxmax()] . . Player_name W. Boly club Wolverhampton Wanderers pos CB Strength 93 Name: 126, dtype: object . Best positioning . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Positioning&#39;]].loc[df3[&#39;Positioning&#39;].idxmax()] . . Player_name S. Agüero club Manchester City pos ST Positioning 94 Name: 5, dtype: object . Best vision . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Vision&#39;]].loc[df3[&#39;Vision&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Vision 94 Name: 0, dtype: object . Best composure . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Composure&#39;]].loc[df3[&#39;Composure&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM Composure 91 Name: 0, dtype: object . Best defensive awareness . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Defensive Awareness&#39;]].loc[df3[&#39;Defensive Awareness&#39;].idxmax()] . . Player_name V. van Dijk club Liverpool pos CB Defensive Awareness 93 Name: 4, dtype: object . Interceptions . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Interceptions&#39;]].loc[df3[&#39;Interceptions&#39;].idxmax()] . . Player_name N. Kanté club Chelsea pos CDM Interceptions 91 Name: 6, dtype: object . Best sliding tackle . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;Sliding Tackle&#39;]].loc[df3[&#39;Sliding Tackle&#39;].idxmax()] . . Player_name A. Wan-Bissaka club Manchester United pos RB Sliding Tackle 90 Name: 57, dtype: object . Best GK reflexes . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;GK Reflexes&#39;]].loc[df3[&#39;GK Reflexes&#39;].idxmax()] . . Player_name H. Lloris club Tottenham Hotspur pos GK GK Reflexes 90 Name: 19, dtype: object . Best GK kicking . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;GK Kicking&#39;]].loc[df3[&#39;GK Kicking&#39;].idxmax()] . . Player_name Ederson club Manchester City pos GK GK Kicking 93 Name: 7, dtype: object . Top player based on wages and salaries . Highest wages . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;wage&#39;]].loc[df3[&#39;wage&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM wage 370000 Name: 0, dtype: object . Highest transfer values . df3[[&#39;Player_name&#39;,&#39;club&#39;,&#39;pos&#39;,&#39;c_value&#39;]].loc[df3[&#39;c_value&#39;].idxmax()] . . Player_name K. De Bruyne club Manchester City pos CAM c_value 1.29e+08 Name: 0, dtype: object . Top earner per club . idx = df3.groupby([&#39;club&#39;])[&#39;wage&#39;].transform(max) == df3[&#39;wage&#39;] top_rated = df3[idx][[&#39;club&#39;,&#39;Player_name&#39;,&#39;pos&#39;,&#39;rating&#39;,&#39;wage&#39;]] top_rated.sort_values(&#39;club&#39;) . . club Player_name pos rating wage . 11 | Arsenal | P. Aubameyang | ST | 87 | 170000.0 | . 165 | Aston Villa | R. Barkley | CAM | 81 | 81000.0 | . 194 | Brighton &amp; Hove Albion | L. Trossard | CAM | 80 | 56000.0 | . 193 | Brighton &amp; Hove Albion | L. Dunk | CB | 80 | 56000.0 | . 192 | Burnley | C. Wood | ST | 78 | 63000.0 | . 6 | Chelsea | N. Kanté | CDM | 88 | 190000.0 | . 63 | Crystal Palace | W. Zaha | CF | 83 | 89000.0 | . 51 | Everton | Allan | CDM | 85 | 115000.0 | . 226 | Fulham | A. Mitrović | ST | 79 | 90000.0 | . 68 | Leeds United | Rodrigo | ST | 82 | 140000.0 | . 20 | Leicester City | J. Vardy | ST | 87 | 170000.0 | . 2 | Liverpool | M. Salah | RW | 90 | 250000.0 | . 3 | Liverpool | S. Mané | LW | 90 | 250000.0 | . 0 | Manchester City | K. De Bruyne | CAM | 91 | 370000.0 | . 18 | Manchester United | Bruno Fernandes | CAM | 88 | 195000.0 | . 173 | Newcastle United | C. Wilson | ST | 78 | 48000.0 | . 231 | Sheffield United | J. Fleck | CM | 77 | 38000.0 | . 230 | Sheffield United | O. Norwood | CM | 77 | 38000.0 | . 92 | Southampton | D. Ings | ST | 81 | 75000.0 | . 10 | Tottenham Hotspur | H. Kane | ST | 88 | 220000.0 | . 365 | West Bromwich Albion | M. Phillips | RM | 74 | 65000.0 | . 97 | West Ham United | S. Haller | ST | 82 | 58000.0 | . 47 | Wolverhampton Wanderers | R. Jiménez | ST | 84 | 140000.0 | . Total wage per club . grouped = df3.groupby(&#39;club&#39;)[&#39;wage&#39;].sum().reset_index() grouped.sort_values(&#39;wage&#39;, ascending=False) . . club wage . 11 | Manchester City | 3552000.0 | . 12 | Manchester United | 2984000.0 | . 10 | Liverpool | 2964000.0 | . 16 | Tottenham Hotspur | 2503000.0 | . 4 | Chelsea | 2371000.0 | . 0 | Arsenal | 2116000.0 | . 6 | Everton | 1835000.0 | . 9 | Leicester City | 1817000.0 | . 7 | Fulham | 1642000.0 | . 8 | Leeds United | 1375000.0 | . 19 | Wolverhampton Wanderers | 1299000.0 | . 5 | Crystal Palace | 1212000.0 | . 17 | West Bromwich Albion | 1103000.0 | . 1 | Aston Villa | 1102000.0 | . 2 | Brighton &amp; Hove Albion | 1049000.0 | . 3 | Burnley | 982000.0 | . 13 | Newcastle United | 978000.0 | . 15 | Southampton | 918000.0 | . 18 | West Ham United | 833000.0 | . 14 | Sheffield United | 652000.0 | . I am not sure the wages in FIFA21 reflect the real wages of these players . Feature engineering . We have 16 different positions. We need to simplify this. . Let&#39;s simplify those to the 4 traditional positions: Goalkeeper, Defender, Midfielder, Forward. For simplicity: . GK: GK | DF: CB, LB, RB, LWB, RWB | MF: CDM, CM, CAM, RM, LM, | FW: RW, LW, CF, ST | . Simplify position . def repos(pos): if pos == &#39;GK&#39;: return &#39;GK&#39; elif pos[-1] == &#39;B&#39;: return &#39;DF&#39; elif pos[-1] == &#39;M&#39;: return &#39;MF&#39; else: return &#39;FW&#39; df3[&#39;pos2&#39;] = df3.apply(lambda x: repos(x[&#39;pos&#39;]), axis=1) . . pos_count = df3[&#39;pos2&#39;].value_counts() pos_count . . MF 240 DF 232 FW 104 GK 67 Name: pos2, dtype: int64 . import plotly.express as px from IPython.display import HTML cat_order = [&#39;GK&#39;,&#39;DF&#39;,&#39;MF&#39;,&#39;FW&#39;] fig = px.bar(pos_count.reindex(cat_order)) fig.update_layout(yaxis_title=&quot;Count&quot;) fig.update_layout(xaxis_title=&quot;Position&quot;) HTML(fig.to_html(include_plotlyjs=&#39;cdn&#39;)) . . . .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/11/09/EPL-FIFA21.html",
            "relUrl": "/2020/11/09/EPL-FIFA21.html",
            "date": " • Nov 9, 2020"
        }
        
    
  
    
  
    
        ,"post2": {
            "title": "[DRAFT - DO NOT SHARE] nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I got from a skeptic to a big fan of literate programming in a month? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://riyan-aditya.github.io//MyBlog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
  
    
  
    
  
    
  
    
        ,"post7": {
            "title": "Classification tutorial following titanic dataset",
            "content": "This project is based on a YouTube video by KenJee The author also provided a link to Kaggle for his notebook This is also based on the popular Kaggle’s Titanic dataset which is used as the introduction for classification problem. . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Ken provided a good overview in each notebook session as well as introductory comment for each cell. Often I read the comment then attempted to code by myself first, then checking with his code afterwards. Good progress for my learning. . Introduction . This Titanic dataset is commonly used as the introduction into the Kaggle competition. With this dataset, we create a model to predict which passengers survived the Titanic shipwreck. . Coding process: . Import data | EDA | Feature Engineering | Data Preprocessing Dropped null values, used dummy variables, imputed data with median and using standard scaler | . | Model building Using: Naive Bayers, Logistic Regression, Decision Tree, K Nearest Neighbour, Random Forest, Support Vector Classifier, Extreme Gradient Boosting, Soft Voting Classifier | . | Model tuning | . Result: . From the base model, the cross validation score achieved was around 75 to 80%. The best model seems to be the Support Vector Classifier model. The ensemble method seems to perform better. . The models were later tuned by using GridSearch. Hyperparameter for model such as RandomForest may take ages to run. Overall, the model performance slightly improved (1 to 3%) after tuning. The Extreme Gradient Boosting model received the highest improvement. Some voting classifier were also conducted using combinations of the tuned models. . At the end, the best performance model with the Kaggle’s test dataset is the hard voting of the tuned model. This achieved 79% score with the test set. . . What I learnt . Insights: . Good idea to do EDA separately between numerical and categorical data. | For categorical data such as cabin or ticket, don’t be afraid to group variables based on certain category (eg: first letter of ticket). This can shorten combinations significantly. | Using Pandas’ get_dummies vs OneHotEncoding. Generally for ML, OHE seems to be better and can be used in pipelines | get_dummies can be applied to a dataframe automatically (it will automatically applied only for the categorical part of the DF). | . | For model like RandomForest, maybe better to start with RandomisedSearchCV then tune with GridSearchCV to cut down searching costs. | . Next step? . I am going to continue with Chapter 4 of the Geron’s book .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/21/Titanic-Kaggle.html",
            "relUrl": "/2020/09/21/Titanic-Kaggle.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Classification tutorial from Chapter 3 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 3 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Minor progress. In managed to become independent while following through the example notebook compared to when I was going through Project 1 (I tried to type some of the codes from scratch). . Introduction . The dataset is the MNIST data that are commonly used as the “hello world” for image classification. Basically, given the input, we are trying to train a model and compare it to the feature label (which is the digit of the image). . . Coding process: . Load data | Start from binary classifier. | Multiclass classifier. | Briefly cover multi label and multi output classification | . Problem: Some of the model takes forever to train (maybe my limited hardware). I ended up using sub datasets (up to 5k data) from the training datasets (60k). Hence, my output wasn’t optimal compared to the example notebook. . Result: Independent of the example, I attempted to make the final prediction by using the SGD model (trained with 10k data) and applied to the test set (the last 1k data imported). . The following is the confusion matrix plot of the test set. Bright colour indicates missclassifed. We can observe that the model confuse: . 3 and 5 (bright white box). | 7 and 9 | Furthermore, many numbers are miss classified as an 8 (colum with an 8). | . . What I learnt . Insights: . For classification problem, it is important to shuffle the dataset. Some algorithm may perform poorly if they get many instances in a row But this is not good for data such as time series data | . | Binary classification using SGDClassifier &amp; RandomForest =&gt; build a number 5 classifier . Performance metrics: accuracy, confusion matrix, precision, recall, F1, Precision Recall curve, ROC curve. . | Consider precision/recall tradeoff. They go together. High precision is not useful for low recall | Use PR curve instead of ROC curve when positive class is rare or care more about FP than FN | . | Multiclass classification =&gt; used SVC and SGD Assess with confusion matrix | Using matshow() can help to visualise and give insight on which classes that are easily miss-classified | . | . Next step? . I am buying this book and will continue on Chapter 4 next week. Really liked the explanation as well as the accompanying notebook in Github. . One of the added exercise is to go through the Titanic dataset on Kaggle. I watched a YouTube video on it before, tried to follow through but unfinished. I will re-attempt this again .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/18/Classification-ML-practice.html",
            "relUrl": "/2020/09/18/Classification-ML-practice.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "End to End ML Project tutorial from Chapter 2 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 2 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. But 95% of the work was following Geron’s. My follow along notebook is here. . Introduction . The dataset we are using is the California Housing Prices dataset based on 1990 California census (see Figure below). We were trying to predict the mean house prices in each district by using regression. . . In summary, it is an end to end ML project: . Importing data from Github | Exploratory Data Analysis | Preparing data Includes: imputing, dealing with categorical data, custom transformer, and applying pipelines | Select and train model The example used: linear regression, decision tree, random forest and support vector regression | Fine tune model / hyperparameter adjustment Via: GridSearch and Randomised search | . Results: . . As seen above, the lowest RMSE is for RandomForest with GridSearch tuning. With this model, the RMSE of the test set is $47.7k . What I learnt . What do i say. I learnt a lot as this is my first time going through the whole process. . Insights: . Random test-train split may not be good if the data is skewed. Test data should have a similar “sub-group” distribution as the full data set =&gt; use StratifiedShuffleSplit | Experiment with attributes. Feature engineering may create a better metrics Eg: In this example, number_of_bedrooms and number_of_household didn’t correlate well with median_house_value. Once we create number_of_rooms/household =&gt; this metric much more correlated to median_house_value | Using SimpleImputer | Using OrdinalEncoder vs OneHotEncoder | Feature engineering via custom transformer | Using Feature Scaling such as StandardScaler | Sparse vs dense matrix. Sparse matrix in OneHotEncoder due to the presence of many 0’s. | Applied the above via pipeline | Using K-fold cross validation (cross_val_score). Beware of training time. | Can do automatic hyperparameter via GridSearchCV and RandomisedSearchCV | Sometimes, knowing the final RMSE isn’t enough. How do we know if this performs better compared to an already deployed model ==&gt; might be good to find the 95% CI | . What I found confusing in this tutorial: . I found it confusing for the author to rename housing to strat_train_set. I would prefer to keep the “train set” variable label throughout the exercise. | . Next step? . Probably continue for classification problem tutorial | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/17/End-to-end-ML-project-practice.html",
            "relUrl": "/2020/09/17/End-to-end-ML-project-practice.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "ML roadmap (By Daniel Bourke)",
            "content": "I like this roadmap from Daniel Bourke’s Youtube video. The roadmap can be accessed here. . . I think this roadmap provides a good overview of how to learn ML. One-stop-shop for everything. The roadmap covers the basic explanation of ML, the available tools and resources and link to the maths behind them if required. . I also like how the video encourages you to approach the learning as a “cook” rather than a “chemist”. Start small. Step-by-step. Learn by doing rather than understanding everything in detail. . Quick overview . Basically, machine learning gives the ability for a machine to learn without being explicitly programmed. . Problems with long lists of rules Eg: it will be complex to program a self-driving car via traditional programming . | Continually changing environment Eg: self-driving car can adapt if there is a new road or traffic sign . | Discovering insights within large collections of data Eg: it will be too much to go to every single transaction manually if you are Amazon . | ML problems . Types of learning for ML: . Supervised learning You have data and labels. The model tries to learn the relationship between data and label. . | Unsupervised learning You have data but no labels. The model tries to find patterns in data without something to reference on. . | Transfer learning Take an existing ML model, then adjust it on your own and use it for your own problem. . | Reinforcement learning When an agent perform an action and being rewarded or penalised based on if the action is favourable or not . | ML problem domains: . Classification The model will use training dataset to learn and then use them to best map the input to the output/label. Eg: classify a mail as spam or not spam . | Regression The model will identify the relationship between the dependent and independent variables. Eg: the price of a stock over time . | Sequence-to-sequence Usually in languages for translation. Eg: Given a sequence in English, translate it to Spanish. . | Clustering Typically an unsupervised problem. Where the model groups data points based on similarity. Eg: Sort a soccer player based on their attributes (striker/defender/goalkeeper etc) . | Dimensionality reduction If you have so much input (100 variables), find the 10 most important variables. Eg: by using PCA (principal component analysis) . | ML process . First, you need to collect some data. It is important here to recognise the type of data you need. Data can be structured data in a table (eg: categorical, numerical, ordinal data etc) or unstructured data (eg: images, speech etc). Remember, rubbish in, rubbish out. . Second, you need to prepare the data. Typical data preparation steps: . Exploratory data analysis (EDA) This process involves understanding your data. Including exploring whether there are outliers and missing data. . | Data pre-processing This step is to prepare your data before the modelling process. Do you fill missing values? | Do we need to do feature encoding (changing categorical data into numbers)? | Do we need to do feature normalisation? | Do we need to do feature engineering? | Do we need to deal with data imbalances? | . | Data splitting For training (70-80% data), validation (10-15% data) and test set (10-15% data) as needed. It is important to not use the test set to tune the model. | . The third step is to train the model. . This is done by choosing an algorithm based on your problem and data. | Beware of underfitting (when the model doesn’t perform as well as you would like based on performance metrics) and overfitting (when the model performs far better on the training set than on the test set) the model. | Typically we overfit first, then reduce through various regularisation technique. | Finally, we can tune various hyperparameters by using validation set data. Eg of hyperparameters: learning rate (usually most important), number of layers, batch size, number of trees etc | . Next, we evaluate the model based on available metrics. There are several considerations here: . How long does a model take to train? | How long does a model need to predict? Eg: It is no good to have a self-driving car model that is 99% accurate but takes 15 seconds to make a prediction | Consider scenario such as what happens if the input data changes | Find out the least confident examples. What does the model usually get wrong? | Consider bias &amp; variance trade-off | . Once we are confident, we can serve the model. We will not know how it performs until we put it out for real. Use different tools whether the final goal is an a mobile app or a web based application. . Finally, we need to continue evaluating the model and retrain the model if needed. The model may change if the data source has changed (such as new road) or data source has been upgraded (such as new hardware used). . What have I learnt from this roadmap? . Good news. I know the basic. I have used Python before and familiar with the libraries. I guess I am closer to intermediate in ML. I have watched or read or have done tutorial covering some of these ML algorithms. . I guess I will start here: . . To do lists (probably in this order): . Do the 3 example projects | Learn/try how to use streamlit to deploy a basic proof of concept | Go through the fast.ai curriculum (seems to be highly recommended by DB, the author) | Test my knowledge in workera.ai | Projects, projects and projects. Probably using Kaggle | SQL maybe? | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/16/ML-roadmap.html",
            "relUrl": "/2020/09/16/ML-roadmap.html",
            "date": " • Sep 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riyan-aditya.github.io//MyBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riyan-aditya.github.io//MyBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}