{
  
    
        "post0": {
            "title": "[DRAFT - DO NOT SHARE] nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I got from a skeptic to a big fan of literate programming in a month? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://riyan-aditya.github.io//MyBlog/codespaces",
            "relUrl": "/codespaces",
            "date": " • Nov 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Regression - Melb house prices",
            "content": "This is my first attempt to do a regression ML problem. I have previously followed a tutorial based on A Geron&#39;s book on a similar project. Time to try by myself. I start through a regression problem because it is the simplest and I am familiar with regression. . I chose house prices data in Melb since I know the place well. Ideally I would have done Jakarta, but I am still learning web scrapping. . House prices data are taken from the Kaggle competition: https://www.kaggle.com/anthonypino/melbourne-housing-market . Things I will attempt to do on this notebook: . As many charts as possible to use interactive visualisation. | Using geopy. Successful to do this given full address, but that takes forever. Doing this in suburb level unsuccessful | . Import python libraries . # download python libraries from datetime import datetime, timedelta import os import glob import wget from bs4 import BeautifulSoup import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import geopandas as gpd import json import plotly.express as px import plotly.graph_objs as go # for offline ploting from plotly.offline import plot, iplot, init_notebook_mode init_notebook_mode(connected=True) from IPython.display import HTML . . Import data . df = pd.read_csv(&#39;MELBOURNE_HOUSE_PRICES_LESS.csv&#39;) df_b = pd.read_csv(&#39;Melbourne_housing_FULL.csv&#39;) . . Exploratory Data Analysis . The kaggle repo indicates that this data was from August 2018 . First, lets see how many properties there are . len(df) . . 63023 . And with an average prices of . round(df.Price.mean(),0) . . 997898.0 . Wow, around 1M? they are expensive . df.columns . Index([&#39;Suburb&#39;, &#39;Address&#39;, &#39;Rooms&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Method&#39;, &#39;SellerG&#39;, &#39;Date&#39;, &#39;Postcode&#39;, &#39;Regionname&#39;, &#39;Propertycount&#39;, &#39;Distance&#39;, &#39;CouncilArea&#39;], dtype=&#39;object&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 63023 entries, 0 to 63022 Data columns (total 13 columns): Suburb 63023 non-null object Address 63023 non-null object Rooms 63023 non-null int64 Type 63023 non-null object Price 48433 non-null float64 Method 63023 non-null object SellerG 63023 non-null object Date 63023 non-null object Postcode 63023 non-null int64 Regionname 63023 non-null object Propertycount 63023 non-null int64 Distance 63023 non-null float64 CouncilArea 63023 non-null object dtypes: float64(2), int64(3), object(8) memory usage: 6.3+ MB . df.describe() . Rooms Price Postcode Propertycount Distance . count | 63023.000000 | 4.843300e+04 | 63023.000000 | 63023.000000 | 63023.000000 | . mean | 3.110595 | 9.978982e+05 | 3125.673897 | 7617.728131 | 12.684829 | . std | 0.957551 | 5.934989e+05 | 125.626877 | 4424.423167 | 7.592015 | . min | 1.000000 | 8.500000e+04 | 3000.000000 | 39.000000 | 0.000000 | . 25% | 3.000000 | 6.200000e+05 | 3056.000000 | 4380.000000 | 7.000000 | . 50% | 3.000000 | 8.300000e+05 | 3107.000000 | 6795.000000 | 11.400000 | . 75% | 4.000000 | 1.220000e+06 | 3163.000000 | 10412.000000 | 16.700000 | . max | 31.000000 | 1.120000e+07 | 3980.000000 | 21650.000000 | 64.100000 | . df.hist(bins=50, figsize=(20,15)) plt.show() . . C: Users Riyan Aditya Anaconda3 lib site-packages pandas plotting _matplotlib tools.py:307: MatplotlibDeprecationWarning: The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead. C: Users Riyan Aditya Anaconda3 lib site-packages pandas plotting _matplotlib tools.py:307: MatplotlibDeprecationWarning: The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead. C: Users Riyan Aditya Anaconda3 lib site-packages pandas plotting _matplotlib tools.py:313: MatplotlibDeprecationWarning: The rowNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().rowspan.start instead. C: Users Riyan Aditya Anaconda3 lib site-packages pandas plotting _matplotlib tools.py:313: MatplotlibDeprecationWarning: The colNum attribute was deprecated in Matplotlib 3.2 and will be removed two minor releases later. Use ax.get_subplotspec().colspan.start instead. . # calculate the correlation matrix corr = df.corr() corr . . Rooms Price Postcode Propertycount Distance . Rooms | 1.000000 | 0.412438 | 0.093666 | -0.048985 | 0.268289 | . Price | 0.412438 | 1.000000 | 0.003112 | -0.060769 | -0.253668 | . Postcode | 0.093666 | 0.003112 | 1.000000 | -0.002557 | 0.500263 | . Propertycount | -0.048985 | -0.060769 | -0.002557 | 1.000000 | 0.014050 | . Distance | 0.268289 | -0.253668 | 0.500263 | 0.014050 | 1.000000 | . Null values in house prices . df.isna().sum() . Suburb 0 Address 0 Rooms 0 Type 0 Price 14590 Method 0 SellerG 0 Date 0 Postcode 0 Regionname 0 Propertycount 0 Distance 0 CouncilArea 0 dtype: int64 . Ok, that is a lot of null values (25%). Let&#39;s remove all rows that have null values. Afterall, we are only interested in property prices . # delete all rows with empty prices df = df[df[&#39;Price&#39;].notna()] . . Prices . import math bin_width= 10**5 nbins = math.ceil((df[&quot;Price&quot;].max() - df[&quot;Price&quot;].min()) / bin_width) fig = px.histogram(df, x=&quot;Price&quot;, nbins=nbins, marginal = &#39;rug&#39;) #fig.update_traces(xbins_size = 10**5) fig.show() . . Looks like most house prices are between 0 to 2M, while there are several outliers where the most expensive is 11.2M!!! . Considering the price histogram is right skewed, perhaps, median is a better indication (especially when we are looking at per suburb) . Average price . df.Price.mean() . 997898.2414882415 . Median price . df.Price.median() . 830000.0 . How many suburbs . df.Suburb.value_counts() . . Reservoir 1067 Bentleigh East 696 Richmond 642 Craigieburn 598 Preston 593 ... Yarra Junction 1 Belgrave Heights 1 Wandin North 1 Belgrave 1 Kalorama 1 Name: Suburb, Length: 370, dtype: int64 . Can i find lat long per address? . # combine address with suburb df[&#39;full_address&#39;] = df[&#39;Address&#39;]+&quot; &quot;+df[&#39;Suburb&#39;]+&#39; Victoria Australia&#39; df[&#39;full_address&#39;][0] . . &#39;49 Lithgow St Abbotsford Victoria Australia&#39; . # additional library for geocode from geopy.geocoders import Nominatim from functools import partial import time . . . # geolocator = Nominatim(user_agent=&quot;melb_address&quot;) # geocode = partial(geolocator.geocode, language=&quot;en&quot;) # aaa = df[&#39;full_address&#39;][0:10].apply(geolocator.geocode).apply(lambda x: (x.latitude, x.longitude)) # aaa . . Hmm. If I run everything to find coordinates for all 64k houses that will take forever . Interactive map of median house price per suburb . Groupby per suburb . Note: attempted to extract coordinate per suburb by using Geopy, but unsuccessful. Extra lat long from a dataset provided by the kaggle OP instead. . df2 = df.copy() df2 = df2[[&#39;Suburb&#39;,&#39;Price&#39;,&#39;Rooms&#39;,&#39;Distance&#39;,&#39;Postcode&#39;]] . . df3 = df2.groupby(&#39;Suburb&#39;).median() df3 = df3.reset_index() df_b2 = df_b.groupby(&#39;Suburb&#39;)[&#39;Lattitude&#39;,&#39;Longtitude&#39;].median().reset_index() df3 = df3.merge(df_b2, on =&#39;Suburb&#39;, how = &#39;inner&#39;) . . px.set_mapbox_access_token(open(&quot;aa.mapbox_token.txt&quot;).read()) fig = px.scatter_mapbox(df3, lat = &#39;Lattitude&#39;,lon = &#39;Longtitude&#39;,color =&#39;Price&#39;, color_continuous_scale=px.colors.sequential.algae, size=&quot;Price&quot;,hover_name=&#39;Suburb&#39;,center = dict(lat=-37.80316 , lon =144.996430 )) fig.show() . Price vs distance? . Are there any correlation between price and distance to CBD? . fig = px.scatter(df, y=&#39;Price&#39; ,x=&#39;Distance&#39;, title=&quot;Distance vs Price&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.show() . . It looks like the further you are from CBD, the lower the median house prices . corr[&#39;Price&#39;] . Rooms 0.412438 Price 1.000000 Postcode 0.003112 Propertycount -0.060769 Distance -0.253668 Name: Price, dtype: float64 . Seems that there are some correlation between price and distance, but not much . 0-10 km . bins = [0,1,2,3,4,5,200] label = [&#39;&lt;1km&#39;,&#39;1-2km&#39;,&#39;2-3km&#39;,&#39;3-4km&#39;,&#39;4-5km&#39;,&#39;the rest&#39;] df_05 = df.groupby(pd.cut(df.Distance, bins))[&#39;Price&#39;].median() plt.bar(label,df_05) plt.ylabel(&#39;Median house price&#39;) . . Text(0, 0.5, &#39;Median house price&#39;) . Interesting. Houses in CBD (perhaps smaller apartments) are relatively cheaper. Note that house sizes are not a factor here, simply average distance . Price vs rooms . df.Rooms.value_counts() . 3 21812 4 11576 2 10674 5 2350 1 1670 6 283 7 36 8 19 10 6 12 2 9 2 31 1 16 1 11 1 Name: Rooms, dtype: int64 . df_rooms = df.groupby(&#39;Rooms&#39;)[&#39;Price&#39;].median() fig = px.bar(df_rooms, title=&quot;Rooms vs Price&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.show() . . Makes sense. Number of rooms increases, the more expensive the house becomes . Price vs type . df.Type.value_counts() . h 34161 u 9292 t 4980 Name: Type, dtype: int64 . We have three type of units: houses, units, townhosues . Do they differ in price? . df_types = df.groupby(&#39;Type&#39;)[&#39;Price&#39;].median() fig = px.bar(df_types, title=&quot;Type vs Price&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.show() . . Houses seems to be the most expensive, followed by town houses, then units . fig = px.box(df,x=&#39;Type&#39;, y = &#39;Price&#39; ,title=&quot;Type vs Price&quot;) fig.update_layout(yaxis_title=&#39;Price ($)&#39;) fig.show() . . price histogram per type . import math bin_width= 10**5 nbins = math.ceil((df[&quot;Price&quot;].max() - df[&quot;Price&quot;].min()) / bin_width) fig = px.histogram(df, x=&quot;Price&quot;, nbins=nbins, color=&#39;Type&#39;,marginal = &#39;rug&#39;, title=&#39;Melbourne houses prices based on unit type&#39;) #fig.update_traces(xbins_size = 10**5) fig.show() . . Selling method . df.Method.value_counts() . S 30624 SP 6480 PI 5940 VB 5024 SA 365 Name: Method, dtype: int64 . S is property sold, SP is property sold prior, PI is property passed in, VB is vendor bid and SA is sold after auction . df_methods = df.groupby(&#39;Method&#39;)[&#39;Price&#39;].median() fig = px.bar(df_methods, title=&quot;Method vs Price&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.show() . . Interesting. There are no significant difference between each method except vendor bid . fig = px.box(df,x=&#39;Method&#39;, y = &#39;Price&#39; ,title=&quot;Method vs Price&quot;) fig.update_layout(yaxis_title=&#39;Price ($)&#39;) fig.show() . . Council area vs price . df_councils = df.groupby(&#39;CouncilArea&#39;)[&#39;Price&#39;,&#39;Distance&#39;].median() df_councils = df_councils.sort_values(&#39;Distance&#39;) fig = px.bar(df_councils, y=&#39;Price&#39;, title=&quot;Council area vs Price (sorted by distance to CBD)&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.show() . . Roughly, the further the council is, the cheaper the houses . Region vs price . df_regions = df.groupby(&#39;Regionname&#39;)[&#39;Price&#39;,&#39;Distance&#39;].median() fig = px.bar(df_regions, y=&#39;Price&#39;, title=&quot;Region vs Price&quot;) fig.update_layout(yaxis_title=&#39;Median price&#39;) fig.update_layout( xaxis={&#39;categoryorder&#39;:&#39;category ascending&#39;}) fig.show() . . Again, metropolitan seems to be more expensive . Data cleaning . There seems to be no need for data cleaning since the data has been cleaned by OP (Tony Pino) before uploading to Kaggle. . Earlier, the houses that have no prices had been removed . Plan to prepare data for ML algo: . Train test split. Going to use stratified sampling. Seems number of bedroom is relevant | No imputing needed since the only missing data was prices and we have deleted rows with no prices | OneHotEncoding for categorical variables (type, selling method and council). Note that suburb has too many variables while region has too few, so council is choosen instead. | Feature Scaling via StandardScaler | Create Pipeline transformer | . corr.Price.sort_values(ascending=False) . Price 1.000000 Rooms 0.412438 Postcode 0.003112 Propertycount -0.060769 Distance -0.253668 Name: Price, dtype: float64 . Looking at the correlation, rooms are an important variable. However, rooms have too many categories. We need to simplify this to maybe 5 to 6 categories (based on the number of rooms) . create room cat for stratified sampling . Rooms_cat categories: . 1: 0-1 room | 2: 1-2 room | 3: 2-3 room | 4: 3-4 room | 5: 4-5 room | 6: &gt;6 room | . df[&#39;Rooms_cat&#39;] = pd.cut(df[&quot;Rooms&quot;], bins=[0, 1, 2, 3, 4, 5, np.inf], labels=[1, 2, 3, 4, 5, 6]) . . df.Rooms_cat.value_counts() . 3 21812 4 11576 2 10674 5 2350 1 1670 6 351 Name: Rooms_cat, dtype: int64 . df.Rooms_cat.hist() . &lt;AxesSubplot:&gt; . Train test split via Stratified sampling . Create test set with 20% of the original data . # apply Stratified Shuffle Split from sklearn.model_selection import StratifiedShuffleSplit split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42) for train_index, test_index in split.split(df, df[&#39;Rooms_cat&#39;]): strat_train_set = df.iloc[train_index] strat_test_set = df.iloc[test_index] . . strat_test_set[&quot;Rooms_cat&quot;].value_counts() / len(strat_test_set) . 3 0.450397 4 0.238980 2 0.220398 5 0.048519 1 0.034479 6 0.007226 Name: Rooms_cat, dtype: float64 . strat_train_set[&quot;Rooms_cat&quot;].value_counts() / len(strat_train_set) . 3 0.450343 4 0.239018 2 0.220384 5 0.048521 1 0.034481 6 0.007252 Name: Rooms_cat, dtype: float64 . strat_train_set.shape, strat_test_set.shape . ((38746, 15), (9687, 15)) . from plotly.subplots import make_subplots fig1 = px.pie(strat_train_set,names=&#39;Rooms_cat&#39;, color_discrete_sequence=px.colors.sequential.Plasma) fig1.update_traces(hovertemplate = &quot;Rooms_cat:%{label} &lt;br&gt;Amount: %{value} &quot;) fig2 = px.pie(strat_test_set,names=&#39;Rooms_cat&#39;,labels = label, color_discrete_sequence=px.colors.sequential.Teal) fig2.update_traces(hovertemplate = &quot;Rooms_cat:%{label} &lt;br&gt;Amount: %{value} &quot;) trace1 = fig1[&#39;data&#39;][0] trace2 = fig2[&#39;data&#39;][0] fig3 = make_subplots(rows=1, cols=2, specs=[[{&#39;type&#39;:&#39;domain&#39;}, {&#39;type&#39;:&#39;domain&#39;}]]) fig3.add_trace(trace1, row=1, col=1) fig3.add_trace(trace2, row=1, col=2) fig3.update_layout(showlegend=False) fig3.update_layout(title_text=&#39;Training vs test dataset breakdown&#39; ,annotations =[dict(text=&#39;Training set&#39;, x=0.18, y=1.1, font_size=20, showarrow=False), dict(text=&#39;Test set&#39;, x=0.82, y=1.1, font_size=20, showarrow=False)]) . . Training and test set now have the same proportion based on number of rooms . Clean test and train set from variables that wont be used . strat_train_set.columns . Index([&#39;Suburb&#39;, &#39;Address&#39;, &#39;Rooms&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Method&#39;, &#39;SellerG&#39;, &#39;Date&#39;, &#39;Postcode&#39;, &#39;Regionname&#39;, &#39;Propertycount&#39;, &#39;Distance&#39;, &#39;CouncilArea&#39;, &#39;full_address&#39;, &#39;Rooms_cat&#39;], dtype=&#39;object&#39;) . strat_test_set.columns . Index([&#39;Suburb&#39;, &#39;Address&#39;, &#39;Rooms&#39;, &#39;Type&#39;, &#39;Price&#39;, &#39;Method&#39;, &#39;SellerG&#39;, &#39;Date&#39;, &#39;Postcode&#39;, &#39;Regionname&#39;, &#39;Propertycount&#39;, &#39;Distance&#39;, &#39;CouncilArea&#39;, &#39;full_address&#39;, &#39;Rooms_cat&#39;], dtype=&#39;object&#39;) . X_train = strat_train_set[[&#39;CouncilArea&#39;,&#39;Distance&#39;,&#39;Rooms_cat&#39;,&#39;Type&#39;,&#39;Method&#39;]] y_train = strat_train_set.Price.copy() X_test = strat_test_set[[&#39;CouncilArea&#39;,&#39;Distance&#39;,&#39;Rooms_cat&#39;,&#39;Type&#39;,&#39;Method&#39;]] y_test = strat_test_set.Price.copy() . . X_train.head() . . CouncilArea Distance Rooms_cat Type Method . 35717 | Monash City Council | 16.7 | 4 | h | S | . 15417 | Yarra City Council | 2.1 | 2 | h | S | . 45499 | Port Phillip City Council | 7.2 | 3 | u | SP | . 728 | Moreland City Council | 5.2 | 3 | h | S | . 32329 | Melbourne City Council | 1.8 | 3 | h | VB | . Split to numerical and categorical to prepare for pipeline . # split to numerical and categorical to prepare for pipeline X_train_num = X_train[[&#39;Rooms_cat&#39;,&#39;Distance&#39;]] X_train_cat = X_train[[&#39;CouncilArea&#39;,&#39;Type&#39;,&#39;Method&#39;]] . . Standard scaler . Apply standard scaler since the variable distance is much larger than rooms . from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler num_pipeline = Pipeline([(&#39;std_scaler&#39;,StandardScaler())]) . . # apply numercial pipeline trial X_train_num_tr = num_pipeline.fit_transform(X_train_num) . . X_train_num_tr . array([[ 1.00920161, 0.53311461], [-1.15756345, -1.40060124], [-0.07418092, -0.72512515], ..., [ 1.00920161, 1.75162048], [-1.15756345, 0.45364683], [-0.07418092, 0.70529479]]) . OHE . Regression requires numerical attribute. so convert all category variables via OHE (Council, Type, Method) . from sklearn.preprocessing import OneHotEncoder cat_encoder = OneHotEncoder() # apply numercial pipeline tria X_train_cat_tr = cat_encoder.fit_transform(X_train_cat) . . X_train_cat_tr.toarray() . array([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.], ..., [0., 0., 0., ..., 0., 1., 0.], [1., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]]) . Apply pipeline . list(X_train_cat), list(X_train_num) . ([&#39;CouncilArea&#39;, &#39;Type&#39;, &#39;Method&#39;], [&#39;Rooms_cat&#39;, &#39;Distance&#39;]) . from sklearn.compose import ColumnTransformer num_attribs = list(X_train_num) cat_attribs = list(X_train_cat) full_pipeline = ColumnTransformer([ (&quot;num&quot;, num_pipeline,num_attribs), (&quot;cat&quot;,cat_encoder,cat_attribs) ]) X_train_prepared = full_pipeline.fit_transform(X_train) . . X_train_prepared . &lt;38746x44 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 193730 stored elements in Compressed Sparse Row format&gt; . X_train_prepared.shape . (38746, 44) . Select and train model . Linear regression . from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X_train_prepared, y_train) . . LinearRegression() . lets try to the first 5 data . # lets try to the first 5 data some_data = X_train.iloc[:5] some_labels = y_train.iloc[:5] some_data_prepared = full_pipeline.transform(some_data) print(&quot;Predictions :&quot;, lin_reg.predict(some_data_prepared)) . . Predictions : [1339783.42238791 1207835.055696 1067805.20722272 1135637.07993019 1525488.71234305] . Compare against the actual values: . print(&quot;Labels:&quot;, list(some_labels)) . Labels: [1220000.0, 1336000.0, 770000.0, 1100000.0, 1400000.0] . MSE &amp; MAE . from sklearn.metrics import mean_squared_error from sklearn.metrics import mean_absolute_error X_train_predictions = lin_reg.predict(X_train_prepared) lin_mse = mean_squared_error(X_train_predictions, y_train) lin_rmse = np.sqrt(lin_mse) print(&quot;RMSE =&quot;,lin_rmse) lin_mae = mean_absolute_error(X_train_predictions, y_train) print(&quot;MAE =&quot;,lin_mae) . . RMSE = 383570.1465041156 MAE = 249536.7459705455 . 380k RMSE is not too bad considering the house prices are in 1-2 M . Decision Tree . from sklearn.tree import DecisionTreeRegressor tree_reg = DecisionTreeRegressor(random_state=42) tree_reg.fit(X_train_prepared, y_train) . . DecisionTreeRegressor(random_state=42) . X_train_pred_tree = tree_reg.predict(X_train_prepared) tree_mse = mean_squared_error(X_train_pred_tree, y_train) tree_rmse = np.sqrt(tree_mse) print(&quot;RMSE =&quot;,tree_rmse) . . RMSE = 273959.98370858666 . Seems like DT has lower RMSE than linear regression . Fine tune model . Better evaluation via cross val . from sklearn.model_selection import cross_val_score scores = cross_val_score(tree_reg, X_train_prepared, y_train, scoring = &quot;neg_mean_squared_error&quot;, cv=5) tree_rmse_scores = np.sqrt(-scores) . . def display_scores(scores): print(&quot;Scores:&quot;, scores) print(&quot;Mean:&quot;, scores.mean()) print(&quot;Standard deviation:&quot;, scores.std()) display_scores(tree_rmse_scores) . . Scores: [324798.10379948 337735.54076006 330201.67755583 331939.44527877 331003.92093241] Mean: 331135.7376653084 Standard deviation: 4129.377526309486 . lin_scores = cross_val_score(lin_reg, X_train_prepared, y_train,scoring=&quot;neg_mean_squared_error&quot;, cv=5) lin_rmse_scores = np.sqrt(-lin_scores) display_scores(lin_rmse_scores) . . Scores: [384501.17139735 387880.98114637 379457.29643916 390707.64209541 377260.56044861] Mean: 383961.5303053785 Standard deviation: 5027.016041937786 . Without cross validation, RMSE: . Linear = 383k | DT = 274k | . With cross validation, RMSE: . Linear = 384k | DT = 331k | . Seems like the DT overfit without cross validation. Whereas the linear model performs very similar . Try RandomForests . from sklearn.ensemble import RandomForestRegressor forest_reg = RandomForestRegressor(n_estimators=100, random_state = 42) forest_reg.fit(X_train_prepared, y_train) . . RandomForestRegressor(random_state=42) . X_train_pred_rf = forest_reg.predict(X_train_prepared) rf_scores = cross_val_score(forest_reg, X_train_prepared, y_train,scoring=&quot;neg_mean_squared_error&quot;, cv=5) rf_rmse_scores = np.sqrt(-rf_scores) display_scores(rf_rmse_scores) . . Scores: [314994.91904795 325593.20672449 323475.15308222 320622.59709846 321517.39368951] Mean: 321240.653928527 Standard deviation: 3561.052811977034 . RF has an RMSE of 321 k . Try SVR . from sklearn.svm import SVR svm_reg = SVR(kernel=&#39;linear&#39;) svm_reg.fit(X_train_prepared, y_train) X_train_pred_svm = svm_reg.predict(X_train_prepared) svm_scores = cross_val_score(svm_reg, X_train_prepared, y_train,scoring=&quot;neg_mean_squared_error&quot;, cv=5) svm_rmse_scores = np.sqrt(-svm_scores) display_scores(svm_rmse_scores) . . Scores: [609787.91630338 613672.87615397 602372.9778416 620121.62071317 595273.34348275] Mean: 608245.746898976 Standard deviation: 8667.653247189583 . RMSE of 608k. terrible . Fine tune via grid search . Since RF is best, lets fine tune just the RF model . from sklearn.model_selection import GridSearchCV param_grid = [{&#39;n_estimators&#39;:[3,10,30,50,100], &#39;max_features&#39;:[2,4,6,8]}, {&#39;bootstrap&#39;:[False],&#39;n_estimators&#39;:[3,30,100], &#39;max_features&#39;:[2,8]}] forest_reg = RandomForestRegressor(random_state=42) grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=&#39;neg_mean_squared_error&#39;, return_train_score=True) grid_search.fit(X_train_prepared, y_train) . . GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), param_grid=[{&#39;max_features&#39;: [2, 4, 6, 8], &#39;n_estimators&#39;: [3, 10, 30, 50, 100]}, {&#39;bootstrap&#39;: [False], &#39;max_features&#39;: [2, 8], &#39;n_estimators&#39;: [3, 30, 100]}], return_train_score=True, scoring=&#39;neg_mean_squared_error&#39;) . cvres = grid_search.cv_results_ for mean_score, params in zip(cvres[&quot;mean_test_score&quot;], cvres[&quot;params&quot;]): print(np.sqrt(-mean_score), params) . . 330383.3627200613 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 323211.32295121235 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 321575.91754455114 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 321673.56777115795 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 50} 321657.04429392307 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 100} 330883.69338457164 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 322791.9775940429 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 321548.77227169415 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 321424.8251180729 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 50} 321240.6222027967 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 100} 330380.9535512754 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 323097.1061609161 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 321193.92971380404 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 321036.8968902753 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 50} 321021.43204733415 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 100} 330488.90810906736 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 322591.0491165248 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 321071.3287094798 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 320913.11811293894 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 50} 320831.9232222634 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 100} 329774.6186165491 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 327830.17787332746 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 327620.6975017789 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 100} 329117.4326502405 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 327158.6264191886 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 327082.4896445029 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 8, &#39;n_estimators&#39;: 100} . Lowest score is: . # lowest score is np.sqrt(-max(cvres[&quot;mean_test_score&quot;])) . . 320831.9232222634 . Grid search helped a little . grid_search.best_params_ . {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 100} . Final model . final_model = grid_search.best_estimator_ . . X_test_prepared = full_pipeline.transform(X_test) final_predictions = final_model.predict(X_test_prepared) final_mse = mean_squared_error(y_test, final_predictions) final_rmse = np.sqrt(final_mse) . . final_rmse . 320133.4700532548 . Final error is similar to the cross validation error. Both are around 320k. THis is not bad considering house prices are in 1-2M range .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/10/23/melb-regression.html",
            "relUrl": "/2020/10/23/melb-regression.html",
            "date": " • Oct 23, 2020"
        }
        
    
  
    
  
    
  
    
        ,"post4": {
            "title": "Classification tutorial following titanic dataset",
            "content": "This project is based on a YouTube video by KenJee The author also provided a link to Kaggle for his notebook This is also based on the popular Kaggle’s Titanic dataset which is used as the introduction for classification problem. . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Ken provided a good overview in each notebook session as well as introductory comment for each cell. Often I read the comment then attempted to code by myself first, then checking with his code afterwards. Good progress for my learning. . Introduction . This Titanic dataset is commonly used as the introduction into the Kaggle competition. With this dataset, we create a model to predict which passengers survived the Titanic shipwreck. . Coding process: . Import data | EDA | Feature Engineering | Data Preprocessing Dropped null values, used dummy variables, imputed data with median and using standard scaler | . | Model building Using: Naive Bayers, Logistic Regression, Decision Tree, K Nearest Neighbour, Random Forest, Support Vector Classifier, Extreme Gradient Boosting, Soft Voting Classifier | . | Model tuning | . Result: . From the base model, the cross validation score achieved was around 75 to 80%. The best model seems to be the Support Vector Classifier model. The ensemble method seems to perform better. . The models were later tuned by using GridSearch. Hyperparameter for model such as RandomForest may take ages to run. Overall, the model performance slightly improved (1 to 3%) after tuning. The Extreme Gradient Boosting model received the highest improvement. Some voting classifier were also conducted using combinations of the tuned models. . At the end, the best performance model with the Kaggle’s test dataset is the hard voting of the tuned model. This achieved 79% score with the test set. . . What I learnt . Insights: . Good idea to do EDA separately between numerical and categorical data. | For categorical data such as cabin or ticket, don’t be afraid to group variables based on certain category (eg: first letter of ticket). This can shorten combinations significantly. | Using Pandas’ get_dummies vs OneHotEncoding. Generally for ML, OHE seems to be better and can be used in pipelines | get_dummies can be applied to a dataframe automatically (it will automatically applied only for the categorical part of the DF). | . | For model like RandomForest, maybe better to start with RandomisedSearchCV then tune with GridSearchCV to cut down searching costs. | . Next step? . I am going to continue with Chapter 4 of the Geron’s book .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/21/Titanic-Kaggle.html",
            "relUrl": "/2020/09/21/Titanic-Kaggle.html",
            "date": " • Sep 21, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Classification tutorial from Chapter 3 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 3 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. My follow along notebook is here. . Minor progress. In managed to become independent while following through the example notebook compared to when I was going through Project 1 (I tried to type some of the codes from scratch). . Introduction . The dataset is the MNIST data that are commonly used as the “hello world” for image classification. Basically, given the input, we are trying to train a model and compare it to the feature label (which is the digit of the image). . . Coding process: . Load data | Start from binary classifier. | Multiclass classifier. | Briefly cover multi label and multi output classification | . Problem: Some of the model takes forever to train (maybe my limited hardware). I ended up using sub datasets (up to 5k data) from the training datasets (60k). Hence, my output wasn’t optimal compared to the example notebook. . Result: Independent of the example, I attempted to make the final prediction by using the SGD model (trained with 10k data) and applied to the test set (the last 1k data imported). . The following is the confusion matrix plot of the test set. Bright colour indicates missclassifed. We can observe that the model confuse: . 3 and 5 (bright white box). | 7 and 9 | Furthermore, many numbers are miss classified as an 8 (colum with an 8). | . . What I learnt . Insights: . For classification problem, it is important to shuffle the dataset. Some algorithm may perform poorly if they get many instances in a row But this is not good for data such as time series data | . | Binary classification using SGDClassifier &amp; RandomForest =&gt; build a number 5 classifier . Performance metrics: accuracy, confusion matrix, precision, recall, F1, Precision Recall curve, ROC curve. . | Consider precision/recall tradeoff. They go together. High precision is not useful for low recall | Use PR curve instead of ROC curve when positive class is rare or care more about FP than FN | . | Multiclass classification =&gt; used SVC and SGD Assess with confusion matrix | Using matshow() can help to visualise and give insight on which classes that are easily miss-classified | . | . Next step? . I am buying this book and will continue on Chapter 4 next week. Really liked the explanation as well as the accompanying notebook in Github. . One of the added exercise is to go through the Titanic dataset on Kaggle. I watched a YouTube video on it before, tried to follow through but unfinished. I will re-attempt this again .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/18/Classification-ML-practice.html",
            "relUrl": "/2020/09/18/Classification-ML-practice.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "End to End ML Project tutorial from Chapter 2 of the Hands-on Machine Learning book",
            "content": "This project is based on Chapter 2 of a book by Aurelien Geron The author also provided a github link for the notebook . I was following the notebook, recreating it, and made some annotations for my own understanding. But 95% of the work was following Geron’s. My follow along notebook is here. . Introduction . The dataset we are using is the California Housing Prices dataset based on 1990 California census (see Figure below). We were trying to predict the mean house prices in each district by using regression. . . In summary, it is an end to end ML project: . Importing data from Github | Exploratory Data Analysis | Preparing data Includes: imputing, dealing with categorical data, custom transformer, and applying pipelines | Select and train model The example used: linear regression, decision tree, random forest and support vector regression | Fine tune model / hyperparameter adjustment Via: GridSearch and Randomised search | . Results: . . As seen above, the lowest RMSE is for RandomForest with GridSearch tuning. With this model, the RMSE of the test set is $47.7k . What I learnt . What do i say. I learnt a lot as this is my first time going through the whole process. . Insights: . Random test-train split may not be good if the data is skewed. Test data should have a similar “sub-group” distribution as the full data set =&gt; use StratifiedShuffleSplit | Experiment with attributes. Feature engineering may create a better metrics Eg: In this example, number_of_bedrooms and number_of_household didn’t correlate well with median_house_value. Once we create number_of_rooms/household =&gt; this metric much more correlated to median_house_value | Using SimpleImputer | Using OrdinalEncoder vs OneHotEncoder | Feature engineering via custom transformer | Using Feature Scaling such as StandardScaler | Sparse vs dense matrix. Sparse matrix in OneHotEncoder due to the presence of many 0’s. | Applied the above via pipeline | Using K-fold cross validation (cross_val_score). Beware of training time. | Can do automatic hyperparameter via GridSearchCV and RandomisedSearchCV | Sometimes, knowing the final RMSE isn’t enough. How do we know if this performs better compared to an already deployed model ==&gt; might be good to find the 95% CI | . What I found confusing in this tutorial: . I found it confusing for the author to rename housing to strat_train_set. I would prefer to keep the “train set” variable label throughout the exercise. | . Next step? . Probably continue for classification problem tutorial | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/17/End-to-end-ML-project-practice.html",
            "relUrl": "/2020/09/17/End-to-end-ML-project-practice.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "ML roadmap (By Daniel Bourke)",
            "content": "I like this roadmap from Daniel Bourke’s Youtube video. The roadmap can be accessed here. . . I think this roadmap provides a good overview of how to learn ML. One-stop-shop for everything. The roadmap covers the basic explanation of ML, the available tools and resources and link to the maths behind them if required. . I also like how the video encourages you to approach the learning as a “cook” rather than a “chemist”. Start small. Step-by-step. Learn by doing rather than understanding everything in detail. . Quick overview . Basically, machine learning gives the ability for a machine to learn without being explicitly programmed. . Problems with long lists of rules Eg: it will be complex to program a self-driving car via traditional programming . | Continually changing environment Eg: self-driving car can adapt if there is a new road or traffic sign . | Discovering insights within large collections of data Eg: it will be too much to go to every single transaction manually if you are Amazon . | ML problems . Types of learning for ML: . Supervised learning You have data and labels. The model tries to learn the relationship between data and label. . | Unsupervised learning You have data but no labels. The model tries to find patterns in data without something to reference on. . | Transfer learning Take an existing ML model, then adjust it on your own and use it for your own problem. . | Reinforcement learning When an agent perform an action and being rewarded or penalised based on if the action is favourable or not . | ML problem domains: . Classification The model will use training dataset to learn and then use them to best map the input to the output/label. Eg: classify a mail as spam or not spam . | Regression The model will identify the relationship between the dependent and independent variables. Eg: the price of a stock over time . | Sequence-to-sequence Usually in languages for translation. Eg: Given a sequence in English, translate it to Spanish. . | Clustering Typically an unsupervised problem. Where the model groups data points based on similarity. Eg: Sort a soccer player based on their attributes (striker/defender/goalkeeper etc) . | Dimensionality reduction If you have so much input (100 variables), find the 10 most important variables. Eg: by using PCA (principal component analysis) . | ML process . First, you need to collect some data. It is important here to recognise the type of data you need. Data can be structured data in a table (eg: categorical, numerical, ordinal data etc) or unstructured data (eg: images, speech etc). Remember, rubbish in, rubbish out. . Second, you need to prepare the data. Typical data preparation steps: . Exploratory data analysis (EDA) This process involves understanding your data. Including exploring whether there are outliers and missing data. . | Data pre-processing This step is to prepare your data before the modelling process. Do you fill missing values? | Do we need to do feature encoding (changing categorical data into numbers)? | Do we need to do feature normalisation? | Do we need to do feature engineering? | Do we need to deal with data imbalances? | . | Data splitting For training (70-80% data), validation (10-15% data) and test set (10-15% data) as needed. It is important to not use the test set to tune the model. | . The third step is to train the model. . This is done by choosing an algorithm based on your problem and data. | Beware of underfitting (when the model doesn’t perform as well as you would like based on performance metrics) and overfitting (when the model performs far better on the training set than on the test set) the model. | Typically we overfit first, then reduce through various regularisation technique. | Finally, we can tune various hyperparameters by using validation set data. Eg of hyperparameters: learning rate (usually most important), number of layers, batch size, number of trees etc | . Next, we evaluate the model based on available metrics. There are several considerations here: . How long does a model take to train? | How long does a model need to predict? Eg: It is no good to have a self-driving car model that is 99% accurate but takes 15 seconds to make a prediction | Consider scenario such as what happens if the input data changes | Find out the least confident examples. What does the model usually get wrong? | Consider bias &amp; variance trade-off | . Once we are confident, we can serve the model. We will not know how it performs until we put it out for real. Use different tools whether the final goal is an a mobile app or a web based application. . Finally, we need to continue evaluating the model and retrain the model if needed. The model may change if the data source has changed (such as new road) or data source has been upgraded (such as new hardware used). . What have I learnt from this roadmap? . Good news. I know the basic. I have used Python before and familiar with the libraries. I guess I am closer to intermediate in ML. I have watched or read or have done tutorial covering some of these ML algorithms. . I guess I will start here: . . To do lists (probably in this order): . Do the 3 example projects | Learn/try how to use streamlit to deploy a basic proof of concept | Go through the fast.ai curriculum (seems to be highly recommended by DB, the author) | Test my knowledge in workera.ai | Projects, projects and projects. Probably using Kaggle | SQL maybe? | .",
            "url": "https://riyan-aditya.github.io//MyBlog/2020/09/16/ML-roadmap.html",
            "relUrl": "/2020/09/16/ML-roadmap.html",
            "date": " • Sep 16, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://riyan-aditya.github.io//MyBlog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://riyan-aditya.github.io//MyBlog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}